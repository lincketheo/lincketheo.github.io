<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-08-04T17:54:24-04:00</updated><id>/feed.xml</id><entry><title type="html">An Elegant Solution to the Producer Consumer Problem</title><link href="/computer%20science/2023/08/04/Flows2.html" rel="alternate" type="text/html" title="An Elegant Solution to the Producer Consumer Problem" /><published>2023-08-04T00:00:00-04:00</published><updated>2023-08-04T00:00:00-04:00</updated><id>/computer%20science/2023/08/04/Flows2</id><content type="html" xml:base="/computer%20science/2023/08/04/Flows2.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>On my quest to understand why flows are so idiomatic to kotlin and elegant in general, I’ve stumbled on the following two observations:</p>

<ol>
  <li>Coroutines / Structured concurrency seamlessly solves the following rule:
<strong>For n blocking calls, the maximally performant program will consume n threads</strong>.</li>
</ol>

<p>Rule (1) is seemingly obvious, until you think about a problem that has an unbounded number of blocking calls. Rule 1 also seems like it relates only to coroutines, but it is partly solved by the introduction of backpressure, which flows (and other patterns) make easier to implement using blocking consumers.</p>

<ol>
  <li>Flows allow us to seperate responsibility within producers / consumers because <strong>the producer within a flow doesn’t need to know what the consumer is doing</strong>, allowing more than one programmer to work on n different consumer functions at the same time.</li>
</ol>

<p>All the rules of <strong>S</strong> in <strong>SOLID</strong> design apply to rule (2). One might say the concept of a flow is more “<em>clean</em>”.</p>

<h2 id="flows---a-cleaner-solution-to-the-producer-consumer-problem">Flows - a Cleaner Solution to the Producer Consumer Problem</h2>

<p>Recently I was working on a problem of loading images into memory while a Machine Learning model was
training. I wanted to load images from memory in an asynchronous way that didn’t violate the
<a href="https://www.reactivemanifesto.org/">Reactivity Manifesto</a>. The basic premises are:</p>

<ol>
  <li>One producer blocks to retrieve data</li>
  <li>One consumer blocks to transform said recieved data</li>
  <li>It’s faster to load n images than 1 image at a time</li>
</ol>

<p>A first naive solution looks something this (I’ll demo this first half in python’s asyncio library because it
has some interesting properties):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>async def produce():
        images = await get_images()
        for image in images:
                await consume(image)

async def consume(image):
        # do blocking consuming stuff

async def get_images():
        # do blocking fetching stuff
</code></pre></div></div>

<p>But recall from the reactive manifesto:</p>

<blockquote>
  <p>The benefits of asynchronous processing would be negated if the communication of back pressure were synchronous</p>
</blockquote>

<p>Although it seems like this code is async (it uses asyncio!), it really isn’t. It’s equivalent to writing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def produce():
        # do blocking fetching stuff
        for image in images:
                # do blocking consuming stuff
</code></pre></div></div>

<p>which is just regular synchronous code! (One of the reasons the async await pattern can sort of trick you into
thinking you’re writing async code). Let’s try again and think about our code executing two threads at the same time
(effectively halving the time it takes to run):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>images = []
async def produce():
        task = asyncio.create_task(get_images())
        for image in images:
                await consume(image)
        images = await task

async def consume(image):
        # do blocking consuming stuff

async def get_images():
        # do blocking fetching stuff
</code></pre></div></div>

<p>We see in this above example that the <em>first</em> time around, images is empty, so the for loop around consume does
nothing, but the <code class="language-plaintext highlighter-rouge">task</code> from asyncio <em>begins fetching the next data before starting to process the current data</em>.
Sort of like the bounded buffer problem loading half the buffer even before the consumer is ready to start consuming
data from the buffer. That means our program execution is cut linearly (if the execution time of fetching
images is the same as consuming them, then the time of the second solution is theoretically
$\frac{1}{2} \times \text{original solution time} + \text{first image fetch}$).</p>

<p>There are other solutions using larger buffers etc, but effectively, if we have two blocking calls and two
threads, we’re maximizing our resources (one of the reasons coroutines / structured concurrency
is so neat, because it maximizes the number of “threads” wrt the number of blocking calls).</p>

<p>A seemingly obvious rule is:</p>

<p><strong>If we have n blocking calls, we can maximize the benefits of asynchronous code by running on n threads at the same time</strong></p>

<p>It’s simple, but we missed it in our first example.</p>

<h2 id="using-flows">Using flows</h2>

<p>The above example is meant to illustrate a solution to backpressure.</p>

<blockquote>
  <p><strong>Backpressure</strong> in software engineering is defined as the ability of a data consumer that cannot keep up with incoming data to send a signal to the data producer to slow down the rate of the data elements. <a href="https://elizarov.medium.com/simple-design-of-kotlin-flow-4725e7398c4c">source</a></p>
</blockquote>

<blockquote>
  <p>Traditional reactive streams design involves a back-channel to request more data from producers as needed. Management of this request protocol leads to notoriously difficult implementations even for simple operators. We do not see any of this complexity in the design of Kotlin flows, nor in the implementation of operators for them, yet Kotlin flows do support back-pressure. <a href="https://elizarov.medium.com/simple-design-of-kotlin-flow-4725e7398c4c">source</a></p>
</blockquote>

<p>The previous example isn’t reactive, but it solves the problem Elizarov highlighted in Flows. Back pressure
management is achieved by blocking the consumer until more data is needed. The producer waits for the consumer to
unblock before it fetches more data (half the buffer in the example above).</p>

<p>Flows don’t offer anything new, other than a more elegant way (using the abstraction that is reactivity)
of implementing the code I wrote above (this is in kotlin now) so that we don’t have to keep solving the problem
of n blockers getting n threads:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val image_producer = flow {
        var images = listOf()
        while (True) {
                val future_images = async(IO, fetch_images)
                image.forEach { emit(it) }
                future_images.onAwait { images = it }
        }
}

val pipeline = image_producer.forEach(consumer)

suspend fun fetch_images() {
        # do blocking recieving stuff
        return images
}

suspend fun consume(image) {
        # do blocking consuming stuff
}
</code></pre></div></div>

<p>This seemingly simple solution seems more complex! But consider more than one consumer:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val image_producer = flow {
        images = listOf()
        while (True) {
                future_images = async(IO) { fetch_images() }
                images.forEach { emit(it) }
                future_images.onAwait { images = it }
        }
}

val mapped = image_producer.map(consumer_1).map(consumer_2)

suspend fun fetch_images() {
        # do blocking recieving stuff
}

suspend fun consume_1(image) {
        # do blocking consuming stuff
        return new_data
}

suspend fun consume_2(image) {
        # do blocking consuming stuff
        return new_data
}
</code></pre></div></div>

<p><strong>The producer doesn’t need any knowledge of what the consumer is going to do with the data</strong>. The producer can just blindly emit data and wait for the consumer to notify that it’s done (_of course, this can be done in other solutions such as the ones using semaphores, flows just feel a bit more intuitive).</p>

<p>In our first example, we would’ve had to add more function calls in the <code class="language-plaintext highlighter-rouge">produce</code> function to consume more than one image or output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>images = []
async def produce():
        task = asyncio.create_task(get_images())
        for image in images:
                new_images1 = await consume_1(image)
                new_images2 = await consume_2(image)
        images = await task

async def consume_1(image):
        # do blocking consuming stuff
        return new_images

async def consume_2(image):
        # do blocking consuming stuff
        return new_images

async def get_images():
        # do blocking fetching stuff
</code></pre></div></div>

<p>Of course, we’re violating the rule that n blocking calls should get n threads and a clever engineer could
probably come up with a clever solution without reactivity, but the point remains that in this non reactive
style of thinking, we have to keep coming up with solutions to delegate our task to multiple threads! So
the real reason reactivity is solving the problem I just mentioned is single responsibility of producers / consumers, letting each producer / consumer get it’s own thread!</p>

<p>Now, we can abstract out the producer and consumer and let multiple software engineers work on
each block of code seperately!</p>]]></content><author><name></name></author><category term="computer science" /><category term="software engineering" /><category term="asynchronous programming" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Notes on Goto/Jump</title><link href="/computer%20science/2023/08/03/Flows.html" rel="alternate" type="text/html" title="Notes on Goto/Jump" /><published>2023-08-03T00:00:00-04:00</published><updated>2023-08-03T00:00:00-04:00</updated><id>/computer%20science/2023/08/03/Flows</id><content type="html" xml:base="/computer%20science/2023/08/03/Flows.html"><![CDATA[<h2 id="notes-on-gotojump">Notes on Goto/Jump</h2>

<p>This is a short one, but I don’t want to rush my fascination with structured concurrency and nonlinear program execution.
This post is inspired heavily by the following <a href="https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/">source</a></p>

<p>Consider the regular python code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Hello World")
</code></pre></div></div>

<p>One key aspect of programming is abstraction over trust. As a programmer, I don’t need to know the implementation details of <code class="language-plaintext highlighter-rouge">print</code>. I don’t need to know that deep down,
python is just one big assembly program and it’s calling gross machine code all over the place, I just need to know that this single function will print the words “Hello World” to the console.
This is abstraction. This is one of Dennis Ritchie’s motivations for writing C. One thing you’ll notice early on when learning to write assembly code is the abundance of jumps; but what 
if in our programming language of choice (python), we relied heavily on jump statements and print was actually defined something like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. # System print
2. def print(s):
3.  if s[0] == '\0':
4.    return
5.  else:
6.    system_print(s[0])
7.    s = s[1:]
8.    jump 3
</code></pre></div></div>

<p>Ugly right? (Of course, python strings aren’t null terminated, and that’s probably not how we jump in python. To be frank, I’m not sure what python’s version of goto is and I have 
never cared to check, even as I write this. I doubt there is one and by principle, I won’t check the internet because I don’t want to know :). It’s ugly, but it still makes sense. 
It’s pretty easy to break, though. How do we define where we jump in our language? I’m jumping to line 3 in the above example, but even languages as dumb as assembly include labels. So 
maybe our code could look something like this so that it’s not entirely broken when we add a new line or comment:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. # System print
2. def print(s):
3. label foo: 
4.  if s[0] == '\0':
5.    return
6. else:
7.    system_print(s[0])
8.    s = s[1:]
9.    jump foo
</code></pre></div></div>

<p>Great. But programmers make mistakes. Say we added a new feature. The author didn’t know that there was already a <code class="language-plaintext highlighter-rouge">foo</code> label defined, so they write a new foo label:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label foo:
really_bad_piece_of_code

# System print
def print(s):
  label foo: 
  if s[0] == '\0':
    return
  else:
    system_print(s[0])
    s = s[1:]
    jump foo
</code></pre></div></div>

<p>Tough luck. There may be guards in our language, and we could keep spouting ways of fixing this problem (scope jumps to functions maybe? But what about within for loops. Isn’t an entire 
program just one big function?). The issue remains that, with jumps, <code class="language-plaintext highlighter-rouge">print</code> is unpredictable. There’s a certain contract that functions will finish after you call them. Otherwise, you
find yourself in gross hot messes.</p>

<p>Everyone who works in the code base needs to know that there’s a foo label in the <code class="language-plaintext highlighter-rouge">print</code> function before they write new stuff and they can’t abstract out control flows. We <em>could</em> trust the writer
of the <code class="language-plaintext highlighter-rouge">print</code> function and assume it acts the way we intend it to, but compile time errors are better than assumptions.</p>

<p>This, and more, is one of the reasons for Dijkstra highlights in <a href="https://www.cs.utexas.edu/~EWD/ewd02xx/EWD249.PDF">Notes On Structured Concurrency</a>. Jump statements
break programmer’s trust.</p>

<h2 id="a-final-note-on-cyclomatic-complexity">A Final Note on Cyclomatic Complexity</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if green:
    if red:
        for i in range(10):
          if blue:
              break
           else:
             if raining:
               print("Raining")
             else:
               print("Not raining")
          if turquoise:
            print("Turquise")
    switch animal:
      case sheep:
        print("No!")
      case possum:
        jump 2
      print("I'm a garbage program")
else:
  print("This isn't very fun to read")
</code></pre></div></div>

<p>Simple code is a pinnacle of software engineering, but most code, under the hood, jumps around a lot. A lot more than you’d think. The difference between an iterative assembly program that literally executes each line of code one after the other and a program as simple as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Hello world")
</code></pre></div></div>

<p>is astronomical. That one print statement contains so many jumps, and each one is abstracted out from the programmer. This is a good thing. Modern compilers optimize out useless jumps, but 
jumps and nonlinear programming execution flows shouldn’t be dismissed. They’re a crucial aspect of any programmer’s repertoire. It’s when we start doing things as ugly as the code I wrote a 
few paragraphs up that jumps and non linear programming execution flows cause problems. What in the world does that program do? I’m sure you could trace it out and simplify it a lot, but
the point is that abstraction is purely for us software engineers. Our computer doesn’t give a crap. It’s all the same code (just with more jumps under the hood). However, we’re writing code
to eliminate bugs and bugs show up when we don’t understand our code. My rant above ties pretty closely to the argument for minimizing Cyclomatic Complexity within our code so it’s easier to follow 
complex lines of logic. <strong>The less things we trust our software engineers to screw up, the better our programs become</strong>.</p>]]></content><author><name></name></author><category term="computer science" /><category term="software engineering" /><category term="asynchronous programming" /><summary type="html"><![CDATA[Notes on Goto/Jump]]></summary></entry><entry><title type="html">A Simple Regex Matcher</title><link href="/computer%20science/2023/06/12/A-Baby-Regex-Matcher.html" rel="alternate" type="text/html" title="A Simple Regex Matcher" /><published>2023-06-12T00:00:00-04:00</published><updated>2023-06-12T00:00:00-04:00</updated><id>/computer%20science/2023/06/12/A-Baby-Regex-Matcher</id><content type="html" xml:base="/computer%20science/2023/06/12/A-Baby-Regex-Matcher.html"><![CDATA[<h2 id="introduction-a-simple-regex-matcher">Introduction: A Simple Regex Matcher</h2>

<p>In this post, I will define a regex and write a simple implementation in <code class="language-plaintext highlighter-rouge">C</code>.</p>

<p>The following is the description of our Regex:</p>

<hr />

<p>Let $\Sigma$ denote the english alphabet,</p>

<ul>
  <li>
    <p>A regex character $a$ can either be ‘.’, ‘*’ or $\in \Sigma$.</p>
  </li>
  <li>
    <p>A Kleene star (‘*’) indicates that the preceeding character (there’s garunteed to be a preceeding character before a Kleene star) can repeat 0 to many times.</p>
  </li>
  <li>
    <p>A ‘.’ can take the place of any character.</p>
  </li>
</ul>

<p>For example:</p>

<p>The following (input, regex) pairs match (where $\epsilon$ represents the empty string):</p>

\[aaa \quad a*\]

\[\epsilon \quad a*\]

<hr />

<h2 id="regular-expressions">Regular Expressions</h2>

<p>Each Regular Expression $r$ constructs a language $L(r)$ of all strings that satisfy $r$.</p>

<p>For example:</p>

\[L(a*) = \{\epsilon, a, aa, aaa, \dots\}\]

<p>A Regular Expression (RE) over some alphabet $\Sigma$ is defined via the following construction rules:</p>

<h3 id="base-rules">Base Rules:</h3>

<ul>
  <li>$\epsilon$ is a RE</li>
  <li>$\forall x \in \Sigma$, x is a RE</li>
</ul>

<h3 id="construction-rules">Construction Rules:</h3>

<ul>
  <li>$(r)|(s)$ is a RE, where</li>
</ul>

\[(r)|(s) = L(r) \cup L(s)\]

<ul>
  <li>$(r)(s)$ is a RE, where</li>
</ul>

\[(r)(s) = \{xy : \forall x,y \quad x \in L(r) \quad y \in L(s)\}\]

<ul>
  <li>$(r)*$ is a RE, where</li>
</ul>

\[(r)* = L(\epsilon) \cup L(r) \cup L(rr) \cup \dots\]

<p>Because we don’t have support for paranthetical expressions in our simple regex, I’m calling our regex a “simple” regex. I’m unsure if there is an official term for Regexes that don’t support grouping.</p>

<h2 id="writing-the-algorithm">Writing the Algorithm</h2>

<p>I’ll do this recursively. I want my “algorithm explained” series to really dive into the thought process of how to solve the problem, but remember that it’s obviously much <em>easier</em> to reason through as someone who’s solved the problem already. This is <em>obviously</em> not how simple my thought process was, rather a set of concrete “aha’s” I went through and their subsequent effect on my code.</p>

<blockquote>
  <p>Side Note: For me writing this blog post, I reasoned through how I could make my code much better / readable. It’s an interesting observation that my first (successful) attempt looked like this:</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bool matches_r(const char *input, const char *regex) {
  // Invalid cases
  if (*regex == '*') {
    printf("Invalid Regex\n");
    return false;
  }

  if (*regex == '\0') {
    return *input == '\0';
  }

  if (*regex != '\0' &amp;&amp; *(regex + 1) != '*') {
    if (*regex == '.') {
      if (*input == '\0') {
        return false;
      }
      return matches_r(input + 1, regex + 1);
    } else {
      if (*input != *regex) {
        return false;
      }
      return matches_r(input + 1, regex + 1);
    }
  }

  if (*regex != '\0' &amp;&amp; *(regex + 1) == '*') {
    bool valid_first = (*input != '\0') &amp;&amp; (*input == *regex || *regex == '.');
    return matches_r(input, regex + 2) || (valid_first &amp;&amp; matches_r(input + 1, regex));
  }

  printf("Can't reach this spot");
  return false;
}
</code></pre></div></div>

<blockquote>
  <p>You’ll see my subsequent attempt at the end, and the obvious reduction in gross logic is a direct consequence of writing this blog post and re doing my work. All that to say, re writing code and writing about your code does wonders!</p>
</blockquote>

<p>Our outline will look something like this:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bool</span> <span class="nf">matches</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">regex</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// todo</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Notice that no matter where you are in the string, you need at least two characters to know what to match. That is, ‘aa’ and ‘a*’ mean two very different things. Therefore, we can eliminate the idea of a context free DSA in our regex.</p>

<p>In order to find all of our recursive conditions, observe that (where ‘a’ represents any character in $\Sigma$):</p>

<ol>
  <li>The first regex character could be a null terminator (‘\0’)</li>
  <li>The first regex character could be ‘.’ and the next character could not be ‘*’
    <ul>
      <li>E.g. “.a” or “.”</li>
    </ul>
  </li>
  <li>The first regex character could be ‘a’ and the next character could not be ‘*’
    <ul>
      <li>E.g. “aa” or “a”</li>
    </ul>
  </li>
  <li>The first regex character could be ‘.’ and the next character could be ‘*’
    <ul>
      <li>E.g. “.*”</li>
    </ul>
  </li>
  <li>The first regex character could be ‘a’ and the next character could be ‘*’
    <ul>
      <li>E.g. “a*”</li>
    </ul>
  </li>
</ol>

<p>And the logic for each case (quotes around “Check” to denote ambiguity):</p>

<ol>
  <li>Check if the input string is also empty - if it is, we’ve matched our string</li>
  <li>“Check” the first character, then run <code class="language-plaintext highlighter-rouge">matches(input + 1, regex + 1)</code></li>
  <li>Similar to (2)</li>
  <li>If the first character doesn’t match, then we can simply check <code class="language-plaintext highlighter-rouge">matches(input, regex + 2)</code> because the kleene star can match 0 characters. Otherwise, if the character matches, then we can run <code class="language-plaintext highlighter-rouge">matches(input + 1, regex)</code>. Also note that if we execute (1-3) correctly, we can ensure we’re not overflowing our buffer.</li>
  <li>Similar to (4)</li>
</ol>

<p>At a high level, here’s our solution:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bool matches_r(const char *input, const char *regex) {
  if (*regex == '\0')
    return *input == '\0';

  bool match = (*regex == *input || *regex == '.');

  if (*(regex + 1) != '*')
    return match &amp;&amp; matches_r(input + 1, regex + 1);

  return matches_r(input, regex + 2) || (match &amp;&amp; matches_r(input + 1, regex));
}
</code></pre></div></div>

<p>But we slightly glossed over case (1). What if input is ‘\0’ and regex matches case 2, we’ll overflow our buffer (mentioned in (4)). _All we need to do is avoid adding anything to <code class="language-plaintext highlighter-rouge">input</code> if <code class="language-plaintext highlighter-rouge">input = '\0'</code>. It looks like <code class="language-plaintext highlighter-rouge">match</code> acts as a short circuit for each case this could happen, so let’s just add the condition there:</p>

<h3 id="the-final-code">The Final Code:</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bool matches_r(const char *input, const char *regex) {
  if (*regex == '\0')
    return *input == '\0';

  bool first_matches = (*input != '\0') &amp;&amp; (*regex == *input || *regex == '.');

  if (*(regex + 1) != '*') 
    return first_matches &amp;&amp; matches_r(input + 1, regex + 1);

  return matches_r(input, regex + 2) || (first_matches &amp;&amp; matches_r(input + 1, regex));
}
</code></pre></div></div>

<h2 id="memoizing">Memoizing</h2>

<p>Classically, in dynamic programming, you’d recognize that you’re executing duplicate code, so you “memoize” that code for later. That is, our problem has <strong>optimal substructure</strong>. I’ll do this problem again in the future while accounting for this property.</p>]]></content><author><name></name></author><category term="computer science" /><category term="regex" /><category term="algorithms" /><summary type="html"><![CDATA[Introduction: A Simple Regex Matcher]]></summary></entry><entry><title type="html">Lp Spaces and Distribution Functions</title><link href="/math/2023/06/12/Distribution-Function.html" rel="alternate" type="text/html" title="Lp Spaces and Distribution Functions" /><published>2023-06-12T00:00:00-04:00</published><updated>2023-06-12T00:00:00-04:00</updated><id>/math/2023/06/12/Distribution-Function</id><content type="html" xml:base="/math/2023/06/12/Distribution-Function.html"><![CDATA[<h2 id="introduction-lp-spaces-and-distribution-functions">Introduction: Lp Spaces and Distribution Functions</h2>

<p>In this post, I will define some more properties of the Borel measure, introduce the Lebesgue measure and introduce the Distribution Function and prove a couple of interesting properties.</p>

<p><strong>Note</strong>: I’m going pretty slow in this foundational work and definitions because:</p>

<p>a) I think it’s important to solidify the basics before moving on to more complex problems (after which, I think it’s ok to speed up)</p>

<p>b) It’s been a while since I was rigorous in mathematics</p>

<p>rest assured, this series will speed up and I will not spend as much time with definitions (just read the book!) and more with intuition and applications.</p>

<h2 id="some-definitions">Some Definitions</h2>

<hr />

<blockquote>
  <p>Intuitively, a <strong>$\sigma$-finite</strong> measure space can be represented by the countable union of subsets $X_n$, each of which has a finite measure.</p>
</blockquote>

<p><strong>Definition</strong>: A measure space $X$ is <strong>$\sigma$-finite</strong> if there exists a sequence of measurable subsets $X_n$ such that:</p>

\[X = \cup_{n=1}^{\infty}X_n\]

\[\mu(X_n) \lt \infty\]

<hr />

<blockquote>
  <p>Intuitively, a <em>characteristic function</em> on a set $X$ (denoted $\chi_{X}(x)$) is equal to $1$ if $x \in X$, otherwise, $0$.</p>
</blockquote>

<p><strong>Definition</strong>: A <strong>finitely simple function</strong> is a finite linear combination of <em>characteristic functions</em> defined on pairwise disjoint sets $B_j$ with complex valued coefficients:</p>

\[\sum_{j=1}^Nc_j\chi_{B_j}\]

<hr />

<p><strong>Definition</strong>: A <strong>countable simple function</strong> is the same as a finitely simple function with $N=\infty$</p>

<hr />

<p><strong>Remark</strong>: Every nonnegative measurable function is the pointwise limit of an increasing sequence of simple functions. These can be finitely simple if the measure space is $\sigma$-finite</p>

<hr />

<p><strong>Definition</strong>:</p>

<ul>
  <li>
    <p>For $p \in (0, \infty)$, $L^p(X, \mu)$ (often abrv $L^p$ when the measure space is known) is the set of all complex valued $\mu$ measurable functions on $X$ whoes modulus to the pth power is integrable.</p>
  </li>
  <li>
    <p>$L^{\infty}(X, \mu)$ is the set of all complex valued $\mu$-measurable functions such that for some $B&gt;0$, $\mu({x : |f(x)| &gt; B}) = 0$.</p>
  </li>
  <li>
    <p>$L^p(\mathbb{R^n})$ is the space $L^p(\mathbb{R}^n, dx)$ where $dx$ is an $n$-dimensional Lebesque measure on $\mathbb{R}^n$.</p>
  </li>
</ul>

<hr />

<p><strong>Definition</strong>: A Borel measure on $\mathbb{R}^n$ is <strong>regular</strong> if it satisfies the two conditions below:</p>

<blockquote>
  <p>Intuitively, a <strong>regular</strong> Borel measure allows you to break around the complexities of open-ness and closed-ness. The set $(0, 1]$ should (intuitively) have the same (Lebesgue) measure as $[0, 1]$. We call this property <strong>regular</strong>. If you took a compact subset (closed and bounded) $K$ (which one may argue has more of an intuitive “measure” than an open set) and expanded it inside $E$ so that it’s measure grows to the largest measure possible inside $E$, then that should be the measure of $E$ (for a regular Borel measure):</p>
</blockquote>

\[\mu(E) = sup\{\mu(K) : K \subset E, \quad K \, compact\}\]

<blockquote>
  <p>We can also think of an open set “closing in on” $E$ until the point in which they have the same measure (for a regular Borel measure):</p>
</blockquote>

\[\mu(E) = inf\{\mu(O) : E \subset O, \quad O \, open\}\]

<hr />

<blockquote>
  <p>Intuitively, we used to take function norms all the time - now we’re just doing so using the Lebesgue integral instead of the Lebesgue measure $dx$:</p>
</blockquote>

<p><strong>Definition</strong>: For $p\in(0,\infty)$, the $L^p$ norm (called the quasi norm for $p &lt; 1$) is definied:</p>

\[\|f\|_{L^p(X, \mu)} = (\int_{X}|f(x)|^pd\mu(x))^{\frac{1}{p}}\]

<p>and,</p>

\[||f||_{L^{\infty}(X, \mu)} = inf\{B &gt; 0 : \mu(\{x : |f(x)| &gt; B\}) = 0\}\]

<blockquote>
  <p>This definition of the $\infty$ norm can intuitively be thought of as pushing a line ($B$) down until it approaches the “top” of $f$ (or the maximum value of $f$).</p>
</blockquote>

<hr />

<h2 id="distribution-functions">Distribution Functions</h2>

<hr />

<p><strong>Definition</strong>: for a measurable function $f$, the <strong>distribution function</strong> of $f$ is $d_f : [0, \infty) \rightarrow [0, \infty]$:</p>

\[d_f(\alpha) = \mu(\{x \in X : |f(x)| &gt; \alpha\})\]

<hr />

<p>Consider the finitely simple function:</p>

\[f(x) = a_3 \quad \text{if } x\in E_3\]

\[f(x) = a_2 \quad \text{if } x\in E_2\]

\[f(x) = a_1 \quad \text{if } x\in E_1\]

\[f(x) = 0 \quad \text{else}\]

<p>With $a_1 &gt; a_2 &gt; a_3$ and $E_j$ pairwise disjoint. This function is just a collection of three “rectangles” (as are all finitely simple functions). For the distribution function $d_f(\alpha)$,</p>

<ul>
  <li>
    <p>Consider when $\alpha \geq a_1$ (so it’s greater than all values of $f(x)$. Then clearly, $d_f(\alpha)=0$ because there is no $x$ that satisfies the set conditions, so the measure of the empty set is 0.</p>
  </li>
  <li>
    <p>Consider when $a_2 \leq \alpha \lt a_1$, (so the line intersects the $a_1$ “rectangle”), then $d_f(\alpha) = \mu(E_1)$.</p>
  </li>
  <li>
    <p>Consider when $a_3 \leq \alpha \lt a_2$, then $d_f(\alpha) = \mu(E_2 \cup E_1) = \mu(E_2) + \mu(E_1)$.</p>
  </li>
  <li>
    <p>Consider when $\alpha \lt a_3$, then $d_f(\alpha) = \mu(E_3 \cup E_2 \cup E_1) = \mu(E_3) + \mu(E_2) + \mu(E_1)$.</p>
  </li>
</ul>

<p><strong>Exercise</strong>: Try this same thing for a more interesting function like $f(x) = x^2$ defined on $[0, 10]$. What about $[0, \infty)$?</p>]]></content><author><name></name></author><category term="math" /><category term="fourier" /><summary type="html"><![CDATA[Introduction: Lp Spaces and Distribution Functions]]></summary></entry><entry><title type="html">Some Silly Paradoxes</title><link href="/math/2023/05/14/paradoxes.html" rel="alternate" type="text/html" title="Some Silly Paradoxes" /><published>2023-05-14T00:00:00-04:00</published><updated>2023-05-14T00:00:00-04:00</updated><id>/math/2023/05/14/paradoxes</id><content type="html" xml:base="/math/2023/05/14/paradoxes.html"><![CDATA[<h2 id="introduction-paradoxes-and-consistency">Introduction: Paradoxes and Consistency</h2>

<p>In this post, I will explore some interesting paradoxes. I’ll probably add to this post in the future, but for now, I just want this post to be about some silly little paradoxes that have shown up in math in the past.</p>

<h2 id="cantors-paradox">Cantor’s Paradox</h2>

<p>Theorem A: The cardinality of a subset (N) of a set (M) is less than or equal to the cardinality of M. E.g., if $N \subset M$, then $\bar{\bar{N}} \leq \bar{\bar{M}}$.</p>

<p>Cantor’s Theorem: The cardinality of a set is strictly less than the cardinality of its power set. E.g., $\bar{\bar{M}} &lt; \bar{\bar{P(M)}}$.</p>

<p>Cantor’s theorem is easy to see from Theorem A. $M$ is a subset of $P(M)$ so $\bar{\bar{M}} \leq P(M)$. The proof of <em>strict</em>-ness (e.g. non-equality) requires a couple more steps. You can start by imagining the set $N$ of all the singleton sets of $M$, which shares the cardinality of $M$.</p>

<p>Cantor’s Paradox: Suppose $M$ is the set of all sets. Then $P(M)$, being a set of all (sub) sets (of M), is an element of $M$. By cantor’s theorem, $\bar{\bar{M}} &lt; \bar{\bar{P(M)}}$, and <strong>not</strong> $\bar{\bar{P(M)}} \geq \bar{\bar{M}}$. However, because $M \subset P(M)$ and by Theorem A, $\bar{\bar{M}} \leq \bar{\bar{P(M)}}$. Hence, a contradiction.</p>

<h2 id="russells-paradox">Russell’s Paradox</h2>

<p>Suppose a barber in a village decides to cut the hair of all the people in the village who <em>do not cut their own hair</em>. Should the barber cut his hair?</p>

<p>Suppose every city in Colorado must have a representative. Every city can have a different representative. Sometimes the representatives of a city do not live in that city. Suppose a law is passed that creates another city S for all the representatives that are not residents of their city to reside. Where should the representative of S reside?</p>

<blockquote>
  <p>Library of Congress, a bibliography of all those bibliographies in the Library of Congress which do not list themselves.</p>
</blockquote>

<p>Consider the set of all sets that are not members of themselves (T):</p>

<ul>
  <li>
    <p>$S = {a, b, c}$ is a member of T</p>
  </li>
  <li>
    <p>$S = {S, a, b}$ is not a member of T</p>
  </li>
</ul>

<p>Is T a member of itself?</p>

<ul>
  <li>
    <p>Suppose Yes: T is a member of itself. Then T is a set that is a member of itself, so by definition, it is not a member of T! This shows that T must <em>not</em> be a member of itself (reductio by absurdum).</p>
  </li>
  <li>
    <p>Suppose No: T is not a member of itself. Then T is not a member of itself, so by definition, it is a member of T! This shows that T <em>must</em> be a member of itself (reductio by absurdum).</p>
  </li>
</ul>

<h2 id="the-epimendies-paradox">The Epimendies Paradox</h2>

<blockquote>
  <p>“Cretans are always liars…” - Epimenides of Crete (Epimendies was a Cretan)</p>
</blockquote>

<p>Certainly, if Epimenides’ statement is true, then Cretans will always tell lies. However, Epimenides’ himself was a Cretan, so he has just shown that he told a truth, invalidating his claim!</p>

<p>So it must be that some Cretans in the past, present, or future tell the truth! However, what if the only thing all Cretans ever said were this sentence? Then all Cretans have ever done is lie!</p>

<p>A more obvious form of the paradox is the sentence:</p>

<p>“This sentence is not true”</p>

<p>If the sentence is true, then the sentence is not true. So by absurdum, the sentence must not be true!</p>

<p>If the sentence is not true, then it must be true. So by absurdum, the sentence must be true!</p>]]></content><author><name></name></author><category term="math" /><category term="paradoxes" /><summary type="html"><![CDATA[Introduction: Paradoxes and Consistency]]></summary></entry><entry><title type="html">Lp Spaces Prerequisites</title><link href="/math/2023/05/14/measure-theory.html" rel="alternate" type="text/html" title="Lp Spaces Prerequisites" /><published>2023-05-14T00:00:00-04:00</published><updated>2023-05-14T00:00:00-04:00</updated><id>/math/2023/05/14/measure-theory</id><content type="html" xml:base="/math/2023/05/14/measure-theory.html"><![CDATA[<h2 id="introduction-some-measure-theory">Introduction: Some Measure Theory</h2>

<p>In this post, I will first lay the basic analysis groundwork needed to understand Borel Sigma Algebras (and of course, I will also explain sigma algebras). Then I will explain the basics of Measure Theory necessary for defining $L^p$ and weak-$L^p$ spaces in subsequent blog posts.</p>

<p>Most of this post’s reference material comes from my own notes on Analysis, but these notes came from <a href="https://www.amazon.com/Analysis-Third-Texts-Readings-Mathematics/dp/9380250649">Terence Tao’s</a> series on analysis. These are very readable introductions to analysis and I highly recommend them.</p>

<h2 id="metric-spaces-and-topologies-a-prerequisite-to-the-borel-sigma-algebra">Metric Spaces and Topologies (A prerequisite to the Borel Sigma Algebra)</h2>

<h3 id="metric-space">Metric Space</h3>

<p>Roughly speaking, a metric is a way to measure distance between two things and map that distance to the real line. It maps an ordered pair of a set $X$ to the real numbers: $d : X \times X \rightarrow \mathbb{R}$ given the following rules:</p>

<p>For any two $x, y \in X$:</p>

<ol>
  <li>Positive: $d(x, y) \geq 0$</li>
  <li>Positive Definiteness: $d(x, y) = 0 \leftrightarrow x = y$</li>
  <li>Commutativity: $d(x, y) = d(y, x)$</li>
  <li>Triangle Inequality: $d(x, y) \leq d(x, z) + d(z, y)$</li>
</ol>

<p>We call metrics with properties 1-3 <em>Semi Metrics</em>. A <em>Metric Space</em> is a set X equipped with a metric $d$, abbreviated $(X, d)$.</p>

<h3 id="topology">Topology</h3>

<p>Roughly speaking, a <em>topology of a metric space</em> is a collection of “balls” that overlap one another. These “balls” are defined via a metric.</p>

<p>For the metric space $(X, d)$, the “Ball centered at $x_0 \in X$ with radius r” is abbreviated $B(x_0, r)$ and is defined:</p>

\[B(x_0, r) := \{y \in X : d(x_0, r) \lt r\}\]

<p>Note the strict inequality. We call this an open set.</p>

<p>For a subset $E \subset X$:</p>

<ul>
  <li>The <em>Complement</em> of $E$ is</li>
</ul>

\[E^c := X - E\]

<ul>
  <li>The <em>Interior</em> of $E$ is</li>
</ul>

\[Int(E) := \{x \in X : \exists r &gt; 0 : B(x, r) \subset E\}\]

<p>Intuitively, an element of $X$ is in the interor if we can make a substantially small “ball” around $x$ that <em>fits</em> inside $E$. Although I haven’t defined border yet, intuitively, think about a value on the border of $E$. No matter how small you make a “ball” surrounding $x$, you won’t be able to fit the entire “Ball” inside $E$!</p>

<ul>
  <li>The <em>Exterior</em> of $E$ is</li>
</ul>

\[Ext(E) := \{x \in X : \exists r &gt; 0 \, st \, B(x, r) \subset E^c\}\]

<p>Intuitively, a point is an exterior point to $E$ if it’s an interior point to $E^c$</p>

<ul>
  <li>The <em>Boundary</em> of $E$ is</li>
</ul>

\[\partial E := X - (Int(E) \cup Ext(E))\]

<p>The <em>Topological Space $X = (X, F)$</em> is a set $X$ with a collection $F$ of subsets of $X$ called <em>open sets</em> that obey the following axioms:</p>

<ol>
  <li>$\emptyset, X$ are open</li>
  <li>The intersection of any finite number of open sets is open.</li>
  <li>The union of any arbitrary number of open sets is open.</li>
</ol>

<p>The collection $F$ is called a <em>topology</em> on X.</p>

<h2 id="sigma-algebras-a-prerequisite-to-measure-spaces">Sigma Algebras (A prerequisite to Measure Spaces)</h2>

<p>We know about power sets of some set $X$ $P(X)$, and some of us know about <em>partitions</em> of a set $X$ (a set of pair-wise disjoint subsets of $X$ that collectively “cover” $X$)</p>

<blockquote>
  <p>Note: n-wise disjoint means if you select any $n$ elements from a set of sets, their intersection is empty.</p>
</blockquote>

\[\{0, 1, 2\}, \{3, 4, 5\}\]

<blockquote>
  <p>is 2-wise disjoint (pair-wise).</p>
</blockquote>

\[\{0, 1, 2\}, \{2, 3, 4\}, \{4, 5, 0\}\]

<blockquote>
  <p>is <em>not</em> 2-wise disjoint, but it is 3-wise disjoint.</p>
</blockquote>

<p>A sigma algebra ($\sigma -$algebra) ($\epsilon$) of a set $X$ is a set of subsets of $X$ that:</p>

<ul>
  <li>Is closed under complement:</li>
</ul>

\[(B \in \epsilon) \rightarrow (B^c \in \epsilon)\]

<ul>
  <li>Is closed under countable unions:</li>
</ul>

\[(\forall_{j \in \mathbb{N}}(B_j \in \epsilon)) \rightarrow ((\cup_{i = 0}^{\infty}B_i) \in \epsilon)\]

<ul>
  <li>Is closed under countable intersections:</li>
</ul>

\[(\forall_{j \in \mathbb{N}}(B_j \in \epsilon)) \rightarrow ((\cap_{i = 0}^{\infty}B_i) \in \epsilon)\]

<ul>
  <li>Contains $\emptyset$ and $X$:</li>
</ul>

\[\emptyset, X \in \epsilon\]

<p>Some examples of trivial sigma algebra’s are:</p>

<ol>
  <li>The power set of $X$, $P(X)$</li>
  <li>The set:</li>
</ol>

\[\{X, \emptyset\}\]

<p>Theorem A: The intersection of a (countably finite or infinite) set of $\sigma$-algebras is also a $\sigma$-algebra.</p>

<p>A $\sigma$-algebra <em>generated</em> by a set $M$ of arbitrary subsets of $X$ is the smallest $\sigma$-algebra that contains $M$ and is denoted in this post as $\sigma(M)$. In other words, the generated $\sigma$-algebra $\sigma(M)$ is the intersection of all $\sigma$-algebra’s that contain $M$ as a subset.</p>

\[\sigma(M) = \cap_{(M \subset A) \land (A \: is \: \sigma-algebra)}A\]

<p>This intersection is not the empty set because $M$ is an element of the powerset $P(X)$, which is a $\sigma$-algebra who shares an intersection with all other $\sigma$-algebras.</p>

<p>For example, consider the sets:</p>

\[X = \{a, b, c, d\} \qquad M = \{\{a\}, \{b\}\}\]

<p>The $\sigma$-algebra generated by $M$ is:</p>

\[=\{\emptyset, X, \{a\}, \{b\}, \{a, b\}, \{b, c, d\}, \{a, c, b\}, \{c, d\}\}\]

<p>All this really means is some $\sigma$-algebra that contains $M$. I got to the above set of sets by first defining:</p>

\[\sigma(M) = \{\emptyset, X...\}\]

<p>By definition of a $\sigma$-algebra, then adding $M$:</p>

\[\sigma(M) = \{\emptyset, X, \{a\}, \{b\}...\}\]

<p>Then completing the $\sigma$-algebra by including complements and unions of sets.</p>

<h2 id="the-borel-sigma-algebra">The Borel Sigma Algebra</h2>

<p>The open sets $F$ of a topologic space $(X, F)$ generate a $\sigma$-algebra called the <strong>Borel $\sigma$ algebra</strong>.</p>

<h2 id="measure-space">Measure Space</h2>

<h3 id="intuition">Intuition</h3>

<p>Intuitively, mathematicians needed a way to “measure” a set. Consider the Lebesque integral as an example motivator. We wish to stray away from the classical Riemann integral because it often breaks with discontinuities. A Riemann integral can be applied to a function with <em>countably finite</em> discontinuities, but consider the function:</p>

\[f(x) = 0 \quad if \quad x \in \mathbb{Q}, \quad 1 \quad if \quad x \in \mathbb{R}-\mathbb{Q}\]

<p>Clearly, this function is not Riemann integrable. What if we had a notion of measuring the set $\mathbb{Q}$ and $\mathbb{R}$? Particularly, if we say there are more real numbers than rational numbers (from Cantor’s argument - see my previous post), we know that this function must be (at least) more than $\frac{1}{2}$. Intuitively, we <em>know</em> there must be some way to “measure” the area of this 2 dimensional “curve” that a Riemann integral just doesn’t support.</p>

<p>The lebesque integral abstracts out the term measurement and starts with the y-axis. When $f(x) = 0$, we know</p>

\[x \in \mathbb{Q}_{[0,1]}\]

<p>and if $f(x) = 1$ we know that</p>

\[x \in \mathbb{R}_{[0,1]}\]

<p>so why not just “measure” the two sets $\mathbb{Q}$ and $\mathbb{R}-\mathbb{Q}$ on the interval $[0, 1]$? In general, a measure space equipts a set with some notion of length, area, volume, etc.</p>

\[\int_{[0,1]}f(x)d\mu(x)\]

<p>Where $\mu(x)$ is the <em>measure</em> of some set element of some $sigma$-alebgra of the measurable space $[0,1]$</p>

<h3 id="definition">Definition</h3>

<p>A measure space is a collection of $(X, \epsilon, \mu)$. Where $X$ is a set, $\epsilon$ is a $\sigma$-algebra of $X$ and $\mu$ is a function $\mu : \epsilon \rightarrow [0, \infty]$ such that:</p>

<ol>
  <li>$\mu(\emptyset) = 0$</li>
  <li>$\mu(\cup_{n=1}^{\infty}B_n) = \sum_{n=1}^{\infty}\mu(B_n)$</li>
</ol>

<p>For any pairwise disjoint elements $B_n \in \epsilon$</p>

<ul>
  <li>$\mu$ is a (positive) <em>measure</em> on $X$</li>
  <li>$B \in epsilon$ is called a <em>measurable set</em></li>
</ul>

<p>A measure space with set $X$ is called $\sigma$-<em>finite</em> if there is some sequence of measurable subsets $X_n \subset X$ such that:</p>

\[X = \cup_{n=1}^{\infty}X_n\]

\[\mu(X_n) \lt \infty\]

<p>A function $f: X \rightarrow \mathbb{R}$ is called <em>measurable</em> if it’s preimage is a measurable set (that is</p>

\[\{x \in X : f(x) &gt; \lambda\}\]

<p>is measurable for all $\lambda \in \mathbb{R}$)</p>

<p>A function $f: X \rightarrow \mathbb{C}$ is called <em>measurable</em> if it’s real and imaginary parts are measurable.</p>]]></content><author><name></name></author><category term="math" /><category term="fourier analysis" /><category term="measure theory" /><summary type="html"><![CDATA[Introduction: Some Measure Theory]]></summary></entry><entry><title type="html">Cantor, Natural Numbers and Formal Systems</title><link href="/math/2023/05/07/nn-systems-cantor.html" rel="alternate" type="text/html" title="Cantor, Natural Numbers and Formal Systems" /><published>2023-05-07T00:00:00-04:00</published><updated>2023-05-07T00:00:00-04:00</updated><id>/math/2023/05/07/nn-systems-cantor</id><content type="html" xml:base="/math/2023/05/07/nn-systems-cantor.html"><![CDATA[<h2 id="introduction-an-assortment-of-concepts">Introduction: An Assortment of Concepts</h2>

<p>In this post, I will explore some properties of natural numbers and abstract systems in mathematics. Reference material belongs in <a href="https://www.amazon.com/Introduction-Metamathematics-Stephen-Cole-Kleene/dp/0923891579">Introduction to Meta Mathematics by Kleene</a> chapter 1.</p>

<h2 id="cantors-diagonalization-argument">Cantor’s Diagonalization Argument</h2>

<p>An <strong>Enumerable Set</strong> $S$ (often called <strong>denumerable</strong> or <strong>countable</strong> but <em>enumerable</em> is the word I will use here to be consistent with Cantor’s theory of sets) is one that can be placed in a 1-1 correspondence with the natural numbers.</p>

<p>Intuitively, you can write all the elements of (S) in a comma-separated list using some algorithmic strategy. You can lay each element out one after the other and cover all of the elements of the set. The set of integers can be written as:</p>

\[0, 1, -1, 2, -2, 3, -3 \dots\]

<p>And we can confidently account for all of the integers in this seemingly obvious pattern. When proving a set is enumerable, a math student is often tasked with finding a function to conclude that there is an injection from a set to the natural numbers; but you may also define an algorithm or pattern for enumerating through each of the elements of the set.</p>

<p>The set of rational numbers can be written as follows:</p>

\[\frac{1}{1}, \frac{2}{1}, \frac{1}{2}, \frac{1}{3}, \frac{2}{2}, \frac{3}{1}, \frac{4}{1}, \frac{3}{2}, \frac{2}{3} \dots\]

<p>See the pattern? It’s tricky to see, but if we arrange the rational numbers in a square matrix with $\frac{1}{1}, \frac{1}{2}, \frac{1}{3}…$ on the top row and $\frac{1}{1}, \frac{2}{1}, \frac{3}{1}…$ on the left column, then we can snake through the diagonals of this matrix to “draw a straight line” through all the rational numbers.</p>

<p>This same layout shows that ordered pairs, ordered triples… can simply be layed out in an n-dimensional matrix and enumerated using a similar strategy shown above, which leads to some interesting claims such as “the set of all polynomials with integral coefficients are enumerable”:</p>

\[a_0x^n + a_1x^{n-1} + ... a_n = 0 \quad a_0 \neq 0\]

<p>can be uniquely identified as:</p>

\[(a_0, a_1... a_n)\]

<p>which is enumerable.</p>

<p>What about the real numbers?</p>

<p>Let’s assume the real numbers in the range $(0, 1)$ (it’s easy to extend this proof to all real numbers) are enumerable and there’s some algorithm for laying out all of them in a column:</p>

\[r_0\]

\[r_1\]

\[r_2\]

<p>…</p>

<p>Now, for each $r_i$ select the $i’th$ decimal place (so if $r_1 = 0.123$ select $2$) and add one to it and place it in the $i’th$ decimal place of a new number $r’$. For example:</p>

\[r_0 = 0.100...\]

\[r_1 = 0.020...\]

\[r_2 = 0.003...\]

\[r' = 0.234\]

<p>We can confidently say that $r’$ is a real number, but does not belong to that enumeration! Meaning the real numbers are not enumerable (e.g., a contradiction).</p>

<p>What did we just show? Earlier we showed that any finite collection of ordered natural numbers was enumerable. Just now, we showed that any <em>infinite</em> collection of ordered natural numbers (where $0.123…$ can be uniquely identified by $(1, 2, 3, …)$) is <em>not</em> enumerable.</p>

<p>In other words, the (infinite) set of all infinitely enumerable sets is not enumerable.</p>

<blockquote>
  <p>More formally: $\omega_1 = $ the set of all countable ordinal numbers</p>
</blockquote>

<p>Whatever form of real analysis you learned, it is easy to see a similarity in this conclusion. For example, consider the following construction of the reals (with lots of pre-definitions left out, e.g. what’s a $LIM$? That’s not important for my point):</p>

<hr />
<p>Definition 5.3.1 (Real numbers). A real number is defined to be an object of the form</p>

\[LIM_{n\rightarrow \infty} a_n\]

<p>where</p>

\[(a_n)_{n=1}^{\infty}\]

<p>is a Cauchy sequence of rational numbers. Two real numbers</p>

\[LIM_{n\rightarrow \infty}a_n \quad and \quad LIM_{n\rightarrow\infty}b_n\]

<p>are said to be equal iff</p>

\[(a_n)_{n=1}^{\infty} \quad and \quad (b_n)_{n=1}^{\infty}\]

<p>are equivalent Cauchy sequences. The set of all real numbers is denoted $\mathbb{R}$.</p>

<hr />

<p>Every definition of the reals in some form of real-analysis will have some “second infinity.” Whereas we only need one natural number to define the next (see the section on natural numbers), we need a sort of “second infinity” to define the real numbers.</p>

<h2 id="cantors-theory-of-naive-sets">Cantor’s theory of (naive) sets</h2>

<p>Cantor defines a “set” and “element”:</p>

<blockquote>
  <p>By a ‘set’ we understand any collection M of definite well-distinguished objects m of our perception or our thought (which are called the ‘elements’ of M) into a while</p>
</blockquote>

<p>Some facts about Cantor’s sets include:</p>

<ul>
  <li>
    <p>Sets include the <em>empty</em> set.</p>
  </li>
  <li>
    <p>Sets have a notion of equality ($M = N$) if they contain the same elements.</p>
  </li>
  <li>
    <p>Sets have a notion of equivalency ($M \sim N$) if there is a 1-1 correspondence between them.</p>
  </li>
</ul>

<p>The <em>cardinal number</em> of a set $M$ is a distinct object $\bar{\bar{M}}$ that is associated in common with all sets (including $M$) that are equivalent to $M$:</p>

\[\bar{\bar{N}} = \bar{\bar{M}} \quad iff \quad N \sim M\]

<p>For now, I think this is all I want to cover on Cantor’s theory of sets in this post (and set theory at all for that matter). Later on, I will describe some notable theorems in set theory (such as Cantor’s theorem and how it leads to Cantor’s paradox), and I will definitely describe Zarmelo Fraenkel set theory eventually, but for the sake of this initial post, I will stop here.</p>

<p>For further reading, you can refer to any well-written book on Set Theory (<a href="https://www.google.com/books/edition/Set_Theory/WTAl997XDb4C?hl=en">Set Theory by Thomas Jech</a> is a good one) to continue further in the study of the theory of sets.</p>

<h2 id="natural-numbers">Natural Numbers</h2>

<p>We define the natural numbers by starting at a single object 0. A natural number can be obtained by taking the <em>successor</em> ($n’$) of any other natural number ($n$). To a computer, all the natural numbers is:</p>

<p>$0, 0’, (0’)’, ((0’)’)’ …$</p>

<p>I use “…” to indicate the continuation of a seemingly obvious pattern. I use $()$ to represent the natural number that is defined by some group of operations happening inside the $()$. From here on out, I will assume $(n’)’$ is the same as $n’’$. A more formal description is through the following propositions (CR meaning “construction rule”):</p>

<ul>
  <li>CR1: 0 is a natural number</li>
  <li>CR2: If $n$ is a natural number, $n’$ is also a natural number</li>
  <li>CR3: The only natural numbers are given by CR1 and CR2.</li>
</ul>

<p>1-3 are an <em>inductive definition</em>. 1-2 represent <em>direct clauses</em>, or instances of the thing we are trying to define. 3 represents the  <em>extremal clause</em>. Distinctness is often useful if we wish for 1-3 to provide utility to areas of mathematics, and it can be defined using the axiom:</p>

<ul>
  <li>
    <p>CR4: For any natural numbers $m$ and $n$ and $m’ = n’$ only if $m = n$.</p>
  </li>
  <li>
    <p>CR5: For any natural number $n, n’\neq0$.</p>
  </li>
</ul>

<p>(Note that Peano ordered 3 as number 5 and pushed 4 and 5 back one spot).</p>

<h2 id="system">System</h2>

<p>A system $S$ of objects is a set $D$ of objects which have established relationships. The natural numbers are a system of type $(D, 0, ‘)$ where $D$ is  a set, $0$ is a member of the set and $’$ is a 1-arity operator on members of $D$. If the objects in $D$ are only known through relationships of the system, the system is called <em>abstract</em>.</p>

<p><em>Abstract</em> systems are defined using some sort of structure. Further specification of what the objects are is a <em>representation</em>. Some examples of <em>representations</em> of the axiomatic system presented in 1-5 are:</p>

<p>The positive natural numbers (just substitute 1 with 0)
The even natural numbers (start with 2 and assume the operator $n’$ is the familiar operation of $n + 2$.</p>

<p>Two <em>representations</em> of the same <em>system</em> are called <em>simply isomorphic</em> (abbrv. <em>isomorphic</em>). That is:</p>

<blockquote>
  <p>Two systems $(D_1, 0_1, ‘_1) and (D_2, 0_2, ‘_2)$ of the type $(D, 0, ‘)$ are simply isomorphic if there exists a 1-1 correspondence between $D_1$ and $D_2$ such that $0_1$ corresponds to $0_2$ and whenever $m_1 \leftrightarrow m_2$, then ${m_1}’_1 \leftrightarrow {m_2}’_2$</p>
</blockquote>

<p>There are two main traditional introductions to systems in mathematics:</p>

<ul>
  <li>
    <p><em>Genetic or constructive</em> method is how we previously constructed the natural numbers.</p>
  </li>
  <li>
    <p><em>Axiomatic or postulational</em> method lays out some propositions that are assumed to be true (<em>axioms</em> or <em>postulates</em>).</p>
  </li>
</ul>

<p>We can also reword 1-5 in our construction of the natural numbers as <em>axioms</em>:</p>

<p>P1: $0 \in D$</p>

<p>P2: $(n \in D) \rightarrow (n’ \in D)$</p>

<p>P3: $(m \in D \land n \in D) \rightarrow (m’ = n’ \rightarrow m = n)$</p>

<p>P4: $(n \in D) \rightarrow (n’ \neq 0)$</p>

<p>P5: $((P \subset D) \land (0 \in P) \land (n \in P \rightarrow n’ \in P)) \rightarrow (P = D)$</p>

<p>Notably, some interesting things happen when we remove axioms. For example, if we remove axiom P4, then the new abstract system is the natural numbers and systems of residues mod $m$ for each positive integer $m$:</p>

<p>residues mod 3:</p>

<p>0, 1, 2, 0, 1, 2, 0, 1, 2…</p>

<p>Or if we replace P4 with P6: $n \in D \rightarrow (n’ \neq n \land n’’ = n)$, then we get residues mod 2:</p>

<p>0, 1, 0, 1, 0, 1….</p>

<p>Axioms may be satisfied by three cases:</p>

<ul>
  <li>No system of objects exists that satisfy the axioms. Such a set of axioms are <em>vacuous</em>. Example:
    <ul>
      <li>P1-P6</li>
    </ul>
  </li>
</ul>

<p>We can show that there is no system that satisfies these axioms. Assume there was such a system. Then $0 \in D$ (P1), so $0’ \in D$ (P2) so $0’’ \in D$ (P2). By P6 and the fact that $0’ \in D$, $0’’ = 0 \land 0’’ \neq 0’$. However, by P4, $0’’ \neq 0$, leading to a contradiction. Therefore, our original claim that there is such a system is false by contradiction.</p>

<ul>
  <li>Non-isomorphic systems exist that satisfy the axioms. That is, the axioms can represent more than one abstract system. Such a set of axioms are <em>non-vacuous</em> and <em>ambiguous</em>. Example:
    <ul>
      <li>P1-P4</li>
      <li>P1-P3, P5</li>
    </ul>
  </li>
</ul>

<p>In the second example, without P4, consider the example <em>representation</em> of the abstract system:</p>

\[0\]

\[0' = 1\]

\[1' = 0\]

<p>(residues mod 2)</p>

<p>Or, similarly, consider the example <em>representation</em>:</p>

\[0\]

\[0' = 1\]

\[1' = 2\]

\[2' = 0\]

<p>(residues mod 3)</p>

<p>These two representations are not isomorphic because there is not a 1-1 correspondence between $D_1 = {0, 1}$ and $D_2 = {0, 1, 2}$.</p>

<ul>
  <li>Exactly one abstract system satisfies the axioms. Such a set of axioms are <em>non-vacuous</em> and <em>categorical</em> Examples:
    <ul>
      <li>P1-P5</li>
      <li>P1-P3, P5, P6</li>
    </ul>
  </li>
</ul>

<p>Sometimes, axioms cannot clearly be put into either category above. For example, until the discovery of non-Euclidean geometry (Lobatchevsky 1829 and Bolyai 1833), Euclid’s axioms (without the parallel postulate) were considered categorical. An excellent story version of this controversy is illustrated in the book Godel Escher Bach by Hofstader, which is an enjoyable read, indeed.</p>

<h2 id="non-enumerable-sets-again">Non-enumerable sets (again)</h2>

<p>Ask yourself if you can list some of the isomorphism representations (see definition later on) of natural numbers defined as a formal system. You might list the following:</p>

<ol>
  <li>The set of natural numbers</li>
  <li>The set of odd integers</li>
  <li>The set of even integers</li>
  <li>The set of rational numbers</li>
  <li>The set of numbers with more than two significant figures</li>
  <li>The set of all primes</li>
  <li>The set of all powers of 7</li>
</ol>

<p>But what about the following set (S): The set of all numbers such that the natural number index in the above list of enumerable sets doesn’t belong to the set that it specifies?</p>

<ul>
  <li>(1) The set of natural numbers</li>
</ul>

<p>Does 1 belong to the set of natural numbers? Yes! So 1 is not an element of S.</p>

<ul>
  <li>(2) The set of odd integers</li>
</ul>

<p>Does 2 belong to the set of even integers? No! So 2 is an element of S.</p>

<ul>
  <li>(3) The set of even integers</li>
</ul>

<p>Does 3 belong to the set of even integers? No! So 3 is an element of S.</p>

<p>And so on…</p>

<p>So far, our set $S = {2, 3, 4, 5, 6 …}$.</p>

<p>Ok, easy, we can just call our new set Set 8. Fixed! But wait, 8 doesn’t belong to Set 8, so we can do the same thing we just did and define Set 9, that <em>does</em> contain the number 8. Fixed! Ah, yet again, 9 is not an element of Set 9.</p>

<p>We can intuitively conclude from this small exercise that it’s impossible to algorithmically describe all of the sets of enumerable sets because we could continue to generate S-type sets (where they describe the set of all numbered indexes before it that are not in their set that they describe). So we have no room to generate the <em>other</em> enumerable sets that could exist. In other words, the set of all sets is a tricky thing to define, and in fact, Cantor’s paradox leads us to conclude that it <em>does not exist</em> (by contradiction).</p>]]></content><author><name></name></author><category term="math" /><category term="natural numbers" /><category term="formal systems" /><category term="set theory" /><summary type="html"><![CDATA[Introduction: An Assortment of Concepts]]></summary></entry><entry><title type="html">Building a Bootloader</title><link href="/computer%20science/2023/04/03/Building-A-Bootloader.html" rel="alternate" type="text/html" title="Building a Bootloader" /><published>2023-04-03T00:00:00-04:00</published><updated>2023-04-03T00:00:00-04:00</updated><id>/computer%20science/2023/04/03/Building-A-Bootloader</id><content type="html" xml:base="/computer%20science/2023/04/03/Building-A-Bootloader.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I’ll build a bootloader from scratch for an Intel x86 32-bit processor in this post. You should have some assembly experience before embarking on your bootloader journey.</p>

<h3 id="notes">Notes</h3>
<ul>
  <li>
    <p>Throughout this tutorial, I’ll point you in the right direction for the two intel manuals I’ve been learning from (<sup id="fnref:fn1" role="doc-noteref"><a href="#fn:fn1" class="footnote" rel="footnote">1</a></sup> and <sup id="fnref:fn2" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>) using the following syntax (section a.b.c <sup id="fnref:fn1:1" role="doc-noteref"><a href="#fn:fn1" class="footnote" rel="footnote">1</a></sup>). These are not comprehensive sections, but they are an excellent place to start looking. As always, the source of truth for this information is not some internet blog written by an amateur software engineer; it’s the intel manuals. However, it is understandable not to want to read each one of their ~500-page manuals thoroughly. Start getting comfortable using them as a last resort if anything on the internet needs to be fully spelled out (I promise my tutorial will have flaws).</p>
  </li>
  <li>
    <p>When I say “Byte 0, 1, 2…” I am assuming bytes start at zero. So “Byte 0” is equivalent to “The first byte .”Another example, “Byte 511,” is the 512th byte.</p>
  </li>
</ul>

<h3 id="what-are-the-responsibilities-of-the-bootloader">What are the responsibilities of the bootloader?</h3>

<p>All the bootloader does is begin executing after being loaded by BIOS, so you can do whatever you want in a bootloader. Technically, there’s no such thing as a defined set of tasks that a bootloader must complete. So once BIOS has handed control off to the bootloader, you have free reign to do whatever you want as an OS developer. However, conceptually, the bootloader traditionally does three main things:</p>

<ol>
  <li>
    <p>It loads whatever code runs <em>after</em> the bootloader (usually the operating system kernel) into memory. We’ll do this by loading the first Sector of our booting disk into memory (because our kernel is so tiny in this tutorial) using the <em>real mode</em> interrupt <a href="https://en.wikipedia.org/wiki/INT_13H#INT_13h_AH=02h:_Read_Sectors_From_Drive">0x13, ah=0x02</a>.</p>
  </li>
  <li>
    <p>General system initialization (initializing the GDT (section 3.4.5<sup id="fnref:fn2:1" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>), stack, segmentation (section 3.3<sup id="fnref:fn2:2" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>), etc.) that you don’t want to do in the kernel. This tutorial initializes the GDT by defining a global <strong>code</strong> and <strong>data</strong> section. We’ll also set up the stack and assume the flat memory model without paging (section 3.2.2<sup id="fnref:fn2:3" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>). It’s my goal to implement paging soon, and I’ll update this tutorial once I’ve set up paging.</p>
  </li>
  <li>
    <p>Switching from <a href="https://en.wikipedia.org/wiki/Real_mode">16 bit real mode</a> to <a href="https://en.wikipedia.org/wiki/Protected_mode">32 bit protected mode</a> (section 9.9.1<sup id="fnref:fn2:4" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>). This step is powerful because it disables all bios interrupts and lets us use all 32 bits and segmentation set up from our GDT. Without protected mode, we can only access 16 bits of address space, or <code class="language-plaintext highlighter-rouge">64 KiB</code> (pretty bad).</p>
  </li>
  <li>
    <p>Jumping to the location of the desired code (usually the kernel). In this tutorial, we’ll write a <em>very</em> simple kernel and store it directly after the bootloader in both memory and disk.</p>
  </li>
</ol>

<h2 id="writing-our-bootloader">Writing Our Bootloader</h2>
<h3 id="step-0-getting-bios-to-recognize-our-bootloader">Step 0: Getting BIOS to Recognize our Bootloader</h3>
<p>Our storage device (flash drive, floppy disk, etc.) stores the bootloader on Sector 1 (512 bytes). BIOS recognizes bootloaders by the magic bytes <code class="language-plaintext highlighter-rouge">0xaa55</code> in bytes 510 and 511. Let’s write an assembly program with <code class="language-plaintext highlighter-rouge">0xaa55</code> in bytes 510 and 511.</p>

<p>First, let’s define the word <code class="language-plaintext highlighter-rouge">0xaa55</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dw 0xaa55
</code></pre></div></div>

<p>We need to place this word on byte 510. We can do that by writing 510 bytes, then executing our command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>times 510 db 0
dw 0xaa55
</code></pre></div></div>

<p>But now what happens? When we boot, there’s no actual code to run! So let’s add an infinite loop. In assembly, an infinite loop looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lbl:
  jmp lbl
</code></pre></div></div>

<p>Which can be shortened to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp $
</code></pre></div></div>

<p>(i.e., jump to my current memory address). All together:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp $
times 510 db 0
dw 0xaa55
</code></pre></div></div>

<p>Now wait a second; we’ve just written an instruction, followed by writing 510 bytes. In Von Neumann’s Architecture, data and code are technically the same thing. So our instructions take up space in our program. The above code looks like this in memory <code class="language-plaintext highlighter-rouge">feeb000000...aa55</code> (where there are 1020 0’s after <code class="language-plaintext highlighter-rouge">feeb</code>. That makes 514 bytes! (510 bytes of 0’s + 2 bytes of <code class="language-plaintext highlighter-rouge">feeb</code> + 2 bytes of <code class="language-plaintext highlighter-rouge">aa55</code> = 514) See for yourself: compile this program:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nasm boot.asm
<span class="nv">$ </span><span class="nb">stat </span>boot
  File: boot
  Size: 514       	Blocks: 8          IO Block: 4096   regular file
...
</code></pre></div></div>

<p>To fix this, we’ll write <code class="language-plaintext highlighter-rouge">510 - number of written bytes</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp $
times 510 ($-$$) db 0
dw 0xaa55
</code></pre></div></div>

<p>Compile this file using nasm:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nasm boot.asm
</code></pre></div></div>

<p>And run it using qemu:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>qemu-system-i386 boot
</code></pre></div></div>

<p>Nothing should happen, but importantly, you won’t crash Qemu. Try changing <code class="language-plaintext highlighter-rouge">510</code> to <code class="language-plaintext highlighter-rouge">511</code> and see what happens.</p>

<p>One last thing. To do anything meaningful, we need to reference the address starting at 0x7c00 because that’s where BIOS loads our program into memory. To do this, you can add the following to your bootloader:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[org 0x7c00]
jmp $
times 510 ($-$$) db 0
dw 0xaa55
</code></pre></div></div>

<h3 id="step-1-loading-our-kernel-code-from-disk">Step 1: Loading our “Kernel” Code from Disk</h3>
<p>Remember that only 512 bytes of our bootloader are loaded. Let’s write a simple “kernel” after our program that prints the character ‘Q’ in real mode and jumps to 0x7e00 (0x7c00 + 512 bytes) - i.e., the code after our bootloader:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp 0x7e00
times 510 - ($-$$) db 0
dw 0xaa55

mov al, 'Q'
call printCharacter
jmp $

printCharacter:
    mov ah, 0x0e ; (teletype output)
    int 0x10
    ret
</code></pre></div></div>

<p>If we compile and run this, nothing happens. That’s because all the code after <code class="language-plaintext highlighter-rouge">dw 0xaa55</code> is on disk, not in memory. We need to load our disk into memory using int 0x13 to fix this. We want to read cylinder (ch) 0, head (DH) 0, and Sector (cl) 2. <em>Sector starts at 1</em>, and our bootloader was on sector 1, so our “kernel” is on sector 2:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov cl, 2
mov dh, 0
mov ch, 0
</code></pre></div></div>

<p>To simplify things, we can load 0 into es (the base of our segment) and 0x7e00 into bx:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov ax, 0
mov es, ax
mov bx, 0x7e00
</code></pre></div></div>

<p>We’ll set <code class="language-plaintext highlighter-rouge">ah</code> to 2 to indicate a read of the disk:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov ah, 0x02
</code></pre></div></div>

<p>And indicate that we want to read one drive (al):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov al, 1
</code></pre></div></div>

<p>(Note: be careful about loading ah and al; they’re part of the same 16-bit register ax).</p>

<p>Lastly, we need to know the drive number. I always imagine that the Bootloader had to call int 0x13 at least once to load our bootloader, so it already did the work of loading the drive number into dl. We don’t actually have a deterministic way of finding out dl. So instead, I like to store dl in memory immediately at the start of booting, then reference that section later in my code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[org 0x7c00]
mov [DRIVE_NUMBER], dl

... other code
mov dl, [DRIVE_NUMBER]
... other code

DRIVE_NUMBER:
   db 0
</code></pre></div></div>

<p>And finally, we call interrupt 0x13:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>int 0x14
</code></pre></div></div>

<p>Putting it all together:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[org 0x7c00]
mov [DRIVE_NUMBER], dl 

call loadSector
jmp 0x7e00

loadSector:
    ; load 1st sector into address 0x7e00
    mov ax, 0
    mov es, ax              
    mov bx, 0x7e00 
    mov ah, 0x02

    ; Cylinder (0), head (0), Sector (2)
    mov cl, 2
    mov dh, 0
    mov ch, 0

    ; Read one Sector
    mov al, 1
    mov dl, [DRIVE_NUMBER]
    
    int 0x13
    ret


DRIVE_NUMBER:
    db 0

times 510 - ($-$$) db 0
dw 0xaa55

mov al, 'Q'
call printCharacter 
jmp $

printCharacter:
    mov ah, 0x0e ; Teletype output: http://www.ctyme.com/intr/rb-0106.htm
    int 0x10     ; call interrupt
    ret
</code></pre></div></div>

<p>Try rerunning it, and you’ll see, as expected, our “kernel code” print a Q to the screen, then infinitely loop.</p>

<h3 id="step-2-set-up-the-gdt">Step 2: Set up the GDT</h3>
<p>In this section, we’ll define our get in a separate file called <code class="language-plaintext highlighter-rouge">gdt.asm</code>. Let’s label our gdt and two sections:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_gdt_start:
   _gdt_null:
      times 8 db 0
   _gdt_code_descriptor
	   ; TODO
   _gdt_data_descriptor
	   ; TODO
gdt_end:
</code></pre></div></div>

<p>Next, let’s define what our <code class="language-plaintext highlighter-rouge">gdt</code> register (<code class="language-plaintext highlighter-rouge">gdtr</code>) should look like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gdtr:
	dw _gdt_end - _gdt_start - 1
	dd _gdt_start
</code></pre></div></div>

<p>And define the prefix to segmented (long) jumps:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>code_seg equ _gdt_code_descriptor - _gdt_start
data_seg equ _gdt_data_descriptor - _gdt_start
</code></pre></div></div>

<p>From (section 3.4.5<sup id="fnref:fn2:5" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">2</a></sup>), we know bits 0-15 consist of the first 16 bits of the segment limit. We want all 4 gigabytes, so our segment limit should be (11111… x20) = 0xfffff (5 f’s). However, the processor puts this value and the later 1/2 byte value of the limit together with this one, so this first 16 bits is only 2 bytes (0xffff) to be combined with (0xf) later on in the gdt:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_gdt_code_descriptor:
    dw 0xffff
</code></pre></div></div>

<p>The following bits (16-31) are the first (of three) parts of the base address, which we will call 0:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DW 0x0000
</code></pre></div></div>

<p>At byte offset 4, bits 0-7, the second half of the base is defined:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>db 0x00
</code></pre></div></div>

<p>Bits 15-12 (decreasing) of byte offset 4 are tricky. They consist of (s, dpl, and p):</p>

<ul>
  <li>p = 1: indicates that this is  a valid segment (if 0, an exception will be thrown)</li>
  <li>dpl = 00: permission level 0 (most privileged)</li>
  <li>s = 1: Code segment</li>
</ul>

<p>Bits 11-8 are the type bits:</p>

<ul>
  <li>e = 1: Indicates that this is an executable segment</li>
  <li>dc = 0: Grows upwards</li>
  <li>rw = 1: Readable</li>
  <li>a = 0: Access bit - keep it 0; the system will set this bit to 1 when this segment is being accessed.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>db 0b10011010
</code></pre></div></div>

<p>Next, bits 23-20 define (avl, l, d/b and g):</p>

<ul>
  <li>g = 1: This segment uses 4 KByte increments</li>
  <li>d/b = 1: 32 Bit protected code segment (instead of 16)</li>
  <li>l = 0: Long mode flag - I was told that this should be 1 if d/b is not 0</li>
  <li>avl = 0: Just used by the processor</li>
</ul>

<p>Bits 19-16 represent the second part of the segment limit (0xf)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>db 0b11001111
</code></pre></div></div>

<p>Finally, the third half of the base offset:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>db 0x00
</code></pre></div></div>

<p>Using the flat memory model, we’ll let both the code and data segment take up the same memory space. The data segment has a similar derivation to the code descriptor, with minor differences.</p>

<p>Putting everything together:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>; GDT describes segments (currently code and data) and their permissions (you can't execute the
; data segment silly)

; References:
; Chapter 3.4.5 (https://www.intel.com/content/www/us/en/content-details/774490/intel-64-and-ia-32-architectures-software-developer-s-manual-volume-3a-system-programming-guide-part-1.html?wapkw=segment%20descriptor)
; And [OSDev wiki](https://wiki.osdev.org/Global_Descriptor_Table)
_gdt_start:
    _gdt_null:
        times 8 db 0 

    ; Segment descriptor - has a complex structure. 
    ; See [Segment Descriptor](https://wiki.osdev.org/Global_Descriptor_Table)
    _gdt_code_descriptor:
        dw 0xffff
        DW 0x0000
        db 0x00
        db 0b10011010
        db 0b11001111
        db 0x00

    _gdt_data_descriptor:
        dw 0xffff
        DW 0x0000
        db 0x00
        db 0b10010010
        db 0b11001111
        db 0x00

_gdt_end:


gdtr:
    dw _gdt_end - _gdt_start - 1
    dd _gdt_start


code_seg equ _gdt_code_descriptor - _gdt_start
data_seg equ _gdt_data_descriptor - _gdt_start

</code></pre></div></div>

<p>We’ll load the gdt in the next section</p>

<h3 id="step-3-entering-protected-mode">Step 3: Entering Protected Mode</h3>
<p>In protected mode, we’ll do a couple of things.</p>
<ol>
  <li>Clear the screen. Here’s a utility function to do that:</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_clear_screen:
    mov ah, 0x00
    mov al, 0x03
    int 0x10
    ret
</code></pre></div></div>

<ol>
  <li>disable interrupts</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cli
</code></pre></div></div>

<ol>
  <li>Load the gdtr that we defined previously</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%include "gdt.asm"

lgdt [gdtr]
</code></pre></div></div>

<ol>
  <li>Enter protected mode by or-ing cr0 with 0x01:</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov eax, cr0
or eax, 0x1
mov cr0, eax
</code></pre></div></div>

<p>We are now in 32-bit protected mode. Let’s set up the stack by executing a <strong>far jump</strong> using the gdt we defined (to the code segment):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp code_seg:_setup

[bits 32]
_setup:
  ...
</code></pre></div></div>

<p>To set up the stack, we’ll set all the stack registers to the data segment (0):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mov ax, data_seg
mov ds, ax
mov ss, ax
mov es, ax
mov fs, ax
mov gs, ax
</code></pre></div></div>

<p>And set the stack starting at address 0x90000:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>; set up the stack base and pointer
mov ebp, 0x90000
mov esp, ebp
</code></pre></div></div>

<p>Finally, we can jump to our kernel!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jmp 0x7e00
</code></pre></div></div>

<h3 id="step-4-wrapping-it-all-together">Step 4: Wrapping it All Together</h3>
<p>In protected mode, we can call int 0x10 to print a character. We need to set the vga to our character explicitly. Putting everything together:</p>

<p><code class="language-plaintext highlighter-rouge">boot.asm</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[org 0x7c00]
mov [DRIVE_NUMBER], dl 

call loadSectors
jmp protected_mode_setup

loadSectors:
    ; load sector into address 0x7e00
    mov ax, 0
    mov es, ax              
    mov bx, 0x7e00 
    mov ah, 0x02

    ; Cylinder (0), head (0), Sector (2)
    mov cl, 2
    mov dh, 0
    mov ch, 0

    ; Read one Sector
    mov al, 1
    mov dl, [DRIVE_NUMBER]
    
    int 0x13
    ret

%include"protected_mode.asm"

DRIVE_NUMBER:
    db 0

times 510 - ($-$$) db 0
dw 0xaa55

mov al, 'Q'
mov ah, 0x0f
mov [0xb8000], ax
jmp $
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">protected_mode.asm</code>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_clear_screen:
    mov ah, 0x00
    mov al, 0x03
    int 0x10
    ret

%include "gdt.asm"

protected_mode_setup:
    call _clear_screen
    cli                     ; 1. disable interrupts

    lgdt [gdtr]             ; 2. load GDT descriptor

    ; Set protection enable bit in cr0 (control register 0)
    ; (you can't just move 1 into cr0, so use a general purpose extended (32-bit) register)
    ; TODO For paging, set bit 31 I think 
    mov eax, cr0
    or eax, 0x1            
    mov cr0, eax
    ; We are now in 32-bit protected mode

    ; Far Jump to the code segment. 
    ; I got confused on this line of code from the OSDevWiki
    ; A far jump takes the form:
    ; jmp &lt;gdt descriptor&gt;:offset
    ; Where the get descriptor is the offset from the get root. For example
    ; the first gdt descriptor would be 0x8 (because null entry)
    ;
    ; I couldn't find the official docs for this, though (TODO)
    jmp code_seg:_protected_mode

[bits 32]
_protected_mode:
    ; Set up the stack and data segments
    mov ax, data_seg
    mov ds, ax
    mov ss, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    ; Set up the stack base and pointer
    mov ebp, 0x90000
    mov esp, ebp

    ; Transfer control to the kernel :)
    jmp 0x7e00 
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">gdt.asm</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>; GDT describes segments (currently code and data) and their permissions (you can't execute the
; data segment silly)

; References:
; Chapter 3.4.5 (https://www.intel.com/content/www/us/en/content-details/774490/intel-64-and-ia-32-architectures-software-developer-s-manual-volume-3a-system-programming-guide-part-1.html?wapkw=segment%20descriptor)
; And [OSDev wiki](https://wiki.osdev.org/Global_Descriptor_Table)
_gdt_start:
    _gdt_null:
        times 8 db 0 

    ; Segment descriptor - has a complex structure. 
    ; See [Segment Descriptor](https://wiki.osdev.org/Global_Descriptor_Table)
    _gdt_code_descriptor:
        ; byte offset 0

        ; bits 0-15: First 16 bits of segment limit
        ; We want all 4 gigabytes, so our segment limit should
        ; be (111111... x20) = 0xfffff (5 f's)
        ; However, the processor puts this value and the later 1/2 byte value of the limit
        ; together with this one, so this one is only 2 bytes (0xffff) to be combined with
        ; (0xf) later on in the gdt
        dw 0xffff
        
        ; bits 16-31 The first (of three) part of the base address (to be concatenated with
        ; the later fields
        DW 0x0000

        ; byte offset 4
        ; bits 0-7: The second half of the base
        db 0x00

        ; bits 15-12 (s, dpl, p)
        ; p = 1 -&gt; indicates that this is a valid segment (if 0, an exception will be thrown)
        ; dpl = 00 -&gt; Permission level 0 (most privileged) Might change this, not sure
        ; s = 1 -&gt; Code or data segment (as opposed to a system segment)

        ; bits 11-8 (Type): 
        ; e = 1 -&gt; Indicates this is an executable segment
        ; dc = 0 -&gt; Indicates that this segment grows upwards
        ; rw = 1 -&gt; Readable
        ; a = 0 -&gt; Access bit: Keep this 0; system sets it to 1 when being accessed
        db 0b10011010

        ; bits 23-20 (avl, l, d/b, g)
        ; g = 1 -&gt; indicates that segment uses 4 KByte increments (ranges from 4KB to 4 GB) 
        ; d/b = 1 -&gt; Indicates 32-bit protected code segment (as opposed to 16)
        ; l = 0 -&gt; Long mode flag; I was told this should be 1 if d/b is not 0
        ; avl = 0 (just used by the processor - no reason it's 0)
        
        ; bits 19-16 Segment limit pt 2 = 0xf (1111)
        db 0b11001111
        
        ; bits : Third half of the base offset
        db 0x00

    _gdt_data_descriptor:
        dw 0xffff
        DW 0x0000
        db 0x00
        db 0b10010010
        db 0b11001111
        db 0x00

_gdt_end:


gdtr:
    dw _gdt_end - _gdt_start - 1
    dd _gdt_start


code_seg equ _gdt_code_descriptor - _gdt_start
data_seg equ _gdt_data_descriptor - _gdt_start
</code></pre></div></div>

<h2 id="appendix">Appendix</h2>
<h3 id="cpu-architecture-jargon-words">CPU architecture jargon words</h3>
<p><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-1-manual.html">Here’s an excellent summary of intel cpu architecture</a> (<strong>chapter 2.1</strong>). Although, often in the intel software development manuals, you can’t just search for jargon terms, so the list below is a bit of a cross-reference for each jargon-y word and a word you can search for in the intel manual:</p>

<p><a href="https://myonlineusb.wordpress.com/2011/06/08/what-is-the-difference-between-i386-i486-i586-i686-i786/">Here’s another nice explanation of i related jargon</a></p>

<p>TLDR, intel’s names make no sense :). These words are frequently misused / loaded, and it’s just best to understand the history.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">x86</code>: (2.1.1) Refers to processors in the 8086 family. (80186 80286 80386 80486…). Usually, it means compatibility with the 80386 32-bit instruction set because 16-bit only is so old (TODO - this isn’t perfectly accurate)</li>
  <li><code class="language-plaintext highlighter-rouge">i686</code>: (2.1.6) Intel686. P6 Family Microarchitecture on the Pentium Pro. One of the 6th generation of <code class="language-plaintext highlighter-rouge">x86</code> processors.</li>
  <li><code class="language-plaintext highlighter-rouge">i386</code>: (2.1.3) Intel386. AKA 80386. First 32-bit (TODO - fact check)</li>
  <li><code class="language-plaintext highlighter-rouge">x86_64</code> The 64-bit instruction set (sometimes called amd64) brother of <code class="language-plaintext highlighter-rouge">x86</code>
    <ul>
      <li>backward compatible with <code class="language-plaintext highlighter-rouge">x86</code> (i.e., <code class="language-plaintext highlighter-rouge">x86</code> instructions can run on <code class="language-plaintext highlighter-rouge">x86_64</code> processors)</li>
    </ul>
  </li>
</ul>

<p><a href="https://www.aliencoders.org/content/basic-information-about-i386-i686-and-x8664-architectures/">32-bit and 64-bit:</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- A 32-bit OS will run on a 32-bit or 64-bit processor without any problem.

- A 32-bit application will run on a 32-bit or 64-bit OS without any problem.

- But a 64-bit application will only run on a 64-bit OS, and a 64-bit OS will only run on a 64-bit processor
</code></pre></div></div>

<h3 id="the-mega-mebi-tera-tebi-confusion">The mega mebi tera tebi… confusion</h3>
<p>Clearing things up because I haven’t seen it stated in the official Intel manuals</p>
<ul>
  <li>An <strong>official</strong> mega byte (MB) is 1000^2 bytes.</li>
  <li>An <strong>official</strong> mebibyte (MiB) is 1024^2 bytes</li>
</ul>

<p>Intel says <code class="language-plaintext highlighter-rouge">MB</code> in their reference manuals because MiB wasn’t introduced until later, and they didn’t want to change all their manuals/references. For all intents and purposes, in the intel manuals, MB means 1024^2^, which conforms with intuition (e.g., 4 GBytes is $4\times2^{30}$ bytes, or $2^2 \times 2^{30}=2^{32}$ bytes, which fits on a 32-bit number).</p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn1" role="doc-endnote">
      <p><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-1-manual.html">Intel Software Development Manual Volume 1A (Basic Architecture)</a></p>
      <ul>
        <li>Not going to help you write code, but useful in learning about the basics of computer stuff</li>
      </ul>
      <p><a href="#fnref:fn1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:fn1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn2" role="doc-endnote">
      <p><a href="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf">Intel Software Development Manual Volume 3A (System Programming Guide Part 1)</a></p>
      <ul>
        <li>Good reference for GDT, protected mode, paging, etc. (The above also talks about paging and segmentation)</li>
      </ul>
      <p><a href="#fnref:fn2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:fn2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:fn2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:fn2:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:fn2:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:fn2:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="Computer Science" /><category term="operating systems" /><category term="bootloader" /><summary type="html"><![CDATA[Introduction I’ll build a bootloader from scratch for an Intel x86 32-bit processor in this post. You should have some assembly experience before embarking on your bootloader journey.]]></summary></entry><entry><title type="html">Kotlin and Error Handling</title><link href="/computer%20science/2023/02/08/Kotlin-Error-Handling.html" rel="alternate" type="text/html" title="Kotlin and Error Handling" /><published>2023-02-08T00:00:00-05:00</published><updated>2023-02-08T00:00:00-05:00</updated><id>/computer%20science/2023/02/08/Kotlin-Error-Handling</id><content type="html" xml:base="/computer%20science/2023/02/08/Kotlin-Error-Handling.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Most of these insights came from <a href="https://elizarov.medium.com/kotlin-and-exceptions-8062f589d07">this</a> blog post and my subsequent attempts to try and implement Elizarov’s suggested patterns.</p>

<p>To illustrate these concepts, I’ll be refactoring a <a href="https://github.com/lincketheo/kotlin-notes/blob/main/src/main/kotlin/exceptions/blogpost/example1/ExceptionsBlogPost.kt">file</a> over and over. Notably <em>only the last block of code will have all the rules applied</em>. The corrected Rule 0 code will not be perfect because it has yet to use Rules 1, 2, etc. (Note: the final fixed code will also not be ideal, either. See my conclusion about how it’s up to you).</p>

<p>Also, lots of these examples refer to SOLID design. There are many opinions on Object Oriented programming and how it can improve, but SOLID doesn’t just apply to object-oriented design. Most statements of the form “Create a new class to …. to comply with abc principle” can easily be replaced with “Create a new function to … to comply with abc principle”. I won’t be going into more detail, but try to keep an open mind on the principle I’m trying to illustrate in each rule and not the specific application to Object Oriented Design.</p>

<p>As always, I prefer rules to lengthy blog posts. These rules should be well-defined and able to be expanded/modified.</p>

<h3 id="rule-0-clean-coding-patterns-make-it-easy-to-handle-exceptions-in-the-right-place">Rule 0: Clean Coding Patterns Make it Easy to Handle Exceptions in the Right Place</h3>

<p><em>Clean code should separate functionality into understandable layers, and the layer responsible for causing the exception should handle it</em>.</p>

<p>For example, consider Android’s layered Repository / DataSource <a href="https://developer.android.com/topic/architecture/data-layer">approach</a>. The single responsibility of a DataSource is to fetch data from a specific location. The single responsibility of a Repository is to combine multiple data sources into one “source of truth.” Naturally, handling data source errors shouldn’t happen in the Repository because that eliminates the rich logic contained inside the Repository.</p>

<p>This rule seems obvious, but it’s easy to forget. Here’s our bad code refactored by moving exception handling to the layer that it applies</p>

<p><a href="https://github.com/lincketheo/kotlin-notes/blob/main/src/main/kotlin/exceptions/blogpost/example2/ExceptionsBlogPost.kt">Example 2</a></p>

<p>Notice how the Repository is the class that handles all of the network and io errors coming in from the two data sources. What if we add another data source to our Repository, or Retrofit throws another exception that isn’t currently handled? The ugly block of exception handling drowns out the rich, meaningful, logical code inside the <code class="language-plaintext highlighter-rouge">refresh</code> function.</p>

<h3 id="rule-1-dont-swallow-exceptions">Rule 1: Don’t swallow exceptions</h3>
<p><em>When you handle an exception, you need to actually do something in your code (usually more than just logging), otherwise, it gets lost</em></p>

<p>This rule comes from a common mistake. We tell ourselves as programmers that we’ll handle all exceptions, but you should only be handling exceptions if you know what to do with them. Otherwise, we sweep fundamental problems with our code under the rug.</p>

<p>We want to do <em>something</em> in our code block with a failed network / local storage call. Something like showing a snack bar or pop-up. This step is where you need to consider the feature you’re implementing. Each feature will have a different set of acceptance requirements. Let’s assume our acceptance requirement is:</p>

<p>When the user refreshes their feed and something (anything) goes wrong, a snack bar pops up</p>

<p>We’ll print to the screen to simplify our example rather than show a snack bar. There are a couple of solutions to this problem. I’ll solve it by exposing a new exception (Be careful about this, I tend to prefer not writing new exceptions - see Rule 3 - but later on, we’ll make this cleaner in Rule 2).</p>

<p><a href="https://github.com/lincketheo/kotlin-notes/blob/main/src/main/kotlin/exceptions/blogpost/example3/ExceptionsBlogPost.kt">Example 3</a></p>

<h3 id="rule-2-consider-treating-exception-handling-as-a-responsibility">Rule 2: Consider Treating Exception Handling as a Responsibility</h3>
<p><em>If you can, treat exception handling logic as a responsibility to make code more meaningful</em></p>

<p>Our exception-handling code is getting complex, so let’s separate exception-handling into a responsibility. In SOLID design, it’s hard to figure out what “responsibility” means in “Single Responsibility.” Certain code smells mean you should start pulling out code blocks into their responsible class:</p>

<ol>
  <li><a href="https://en.wikipedia.org/wiki/Cyclomatic_complexity">Cyclomatic Complexity</a> - When there are many ways to exit your code block, pull in multiple classes to handle each case.</li>
  <li>Low <a href="https://en.wikipedia.org/wiki/Cohesion_(computer_science)">Code Cohesion</a> - When two blocks of code are entirely disjoint from one another but exist in the same block, pull each block into a separate class.</li>
  <li>Repeating blocks of code (even if it’s not an exact repeat) - When two blocks look similar, you can sometimes pull that pattern into another subroutine.</li>
</ol>

<p>The following block of code appears (almost identically) twice in our code:</p>

<div class="language-kotlin highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span> <span class="p">{</span>
	<span class="nf">doSomethingWithADataSource</span><span class="p">()</span>
<span class="p">}</span> <span class="k">catch</span><span class="p">(</span><span class="n">e</span><span class="p">:</span> <span class="nc">SomeDataException</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">throw</span> <span class="nc">AppDataSourceException</span><span class="p">(</span><span class="o">..</span><span class="p">.)</span>
<span class="p">}</span>
</code></pre></div></div>

<p>From Rule 0, we agreed that we’d handle all our data source exceptions in the data source layer, and in Rule 1, we decided we’d throw a single exception type for any data source problem. Let’s define a new function, <code class="language-plaintext highlighter-rouge">attemptDataSourceOperation</code> that makes us stay loyal to those two patterns we said we’d follow.</p>

<div class="language-kotlin highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">inline</span> <span class="k">fun</span> <span class="p">&lt;</span><span class="nc">T</span><span class="p">&gt;</span> <span class="nf">attemptDataSourceOperation</span><span class="p">(</span>
    <span class="n">catchExceptionTypes</span><span class="p">:</span> <span class="nc">Set</span><span class="p">&lt;</span><span class="nc">KClass</span><span class="p">&lt;</span><span class="k">out</span> <span class="nc">Exception</span><span class="p">&gt;&gt;,</span>
    <span class="n">onFailExposedMessage</span><span class="p">:</span> <span class="nc">String</span><span class="p">,</span>
    <span class="n">block</span><span class="p">:</span> <span class="p">()</span> <span class="p">-&gt;</span> <span class="nc">T</span>
<span class="p">):</span> <span class="nc">T</span> <span class="p">{</span>
    <span class="k">try</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nf">block</span><span class="p">()</span>
    <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="n">e</span><span class="p">:</span> <span class="nc">Exception</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">catchExceptionTypes</span><span class="p">.</span><span class="nf">find</span> <span class="p">{</span> <span class="n">it</span><span class="p">.</span><span class="nf">isInstance</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="p">}</span><span class="o">?.</span><span class="nf">let</span> <span class="p">{</span>
            <span class="k">throw</span> <span class="nc">AppDataSourceException</span><span class="p">(</span>
                <span class="n">message</span> <span class="p">=</span> <span class="n">onFailExposedMessage</span><span class="p">,</span>
                <span class="n">cause</span> <span class="p">=</span> <span class="n">e</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">}</span>
        <span class="k">throw</span> <span class="n">e</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now, if we want to catch exceptions in the data source and expose them as <code class="language-plaintext highlighter-rouge">AppDataSourceExceptions</code>, we can write the following code:</p>

<div class="language-kotlin highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fun</span> <span class="nf">doSomethingInTheDataSource</span><span class="p">():</span> <span class="nc">Data</span> <span class="p">{</span>
	<span class="k">return</span> <span class="nf">attemptDataSourceOperation</span><span class="p">(</span>
		<span class="n">catchExceptionTypes</span> <span class="p">=</span> <span class="nf">setOf</span><span class="p">(</span><span class="nc">The</span> <span class="n">exception</span> <span class="n">types</span> <span class="nc">I</span> <span class="n">want</span> <span class="n">to</span> <span class="k">catch</span><span class="p">),</span>
		<span class="n">onFailExposedMessage</span> <span class="p">=</span> <span class="s">"doSomethingInTheDataSource failed!"</span><span class="p">,</span>
	<span class="p">)</span> <span class="p">{</span>
		<span class="nf">attemptDataSourceOperationThatFails</span><span class="p">()</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/lincketheo/kotlin-notes/blob/main/src/main/kotlin/exceptions/blogpost/example4/ExceptionsBlogPost.kt">Example 4</a></p>

<p>Now this might be a bit of overkill. However, it stays loyal to our architecture. The class <code class="language-plaintext highlighter-rouge">AppDataSourceException</code> exists, so any time a data source throws, an <code class="language-plaintext highlighter-rouge">AppDataSourceException</code> is probably thrown. However, this might be different for your application. Specific DataSource exceptions could cause other effects for the end user, where the single <code class="language-plaintext highlighter-rouge">AppDataSourceException</code> class is probably not the way to go. However, for our ACs, any data source exception was treated the same.</p>

<h3 id="rule-3---prefer-not-to-write-new-exceptions">Rule 3 - Prefer Not to Write New Exceptions</h3>
<p><em>Don’t write new exceptions unless they should actually happen while in production</em></p>

<p>Exceptions we handle should only highlight something we can do something about. That means only writing new exceptions when it’s something that <em>can</em> go wrong in your application. A network can fail, and a user can have invalid credentials. You don’t need to handle exceptions if your code isn’t meant to throw that exception. For example, we usually don’t need to handle <code class="language-plaintext highlighter-rouge">IllegalArgumentException</code>’s because, as programmers, we shouldn’t be writing code using IllegalArguments! <strong>If an exception highlights a bug in your code, let it go uncaught</strong>. We can take measures to hide this fact from the user (such as writing a global uncaught exception handler that notifies the developers that something went wrong with the stack trace and tells the user with a generic “Something went wrong” message so that we don’t crash the app). However, only write logic to catch sound logic.</p>

<p>Sound logical exceptions are things that <em>can go wrong</em>. In my experience, these are the few times I will write a new exception (I am missing out on many cases, but many cases boil down to these rules):</p>

<ol>
  <li>Our ACs glob multiple exceptions into one. For example, our AC is “If anything goes wrong on a refresh related to network or local operations, tell the user something went wrong.” We know that our refresh call could produce some HTTP exceptions, IO Exceptions, etc., so we can define a single exception, <code class="language-plaintext highlighter-rouge">AppDataSourceException</code>, that our refresh call handles.</li>
  <li>Our code encounters an unexpected problem. For example, a mobile developer has no way (in theory) to catch problems with their API so one might write an <code class="language-plaintext highlighter-rouge">UnexpectedAPIResponse</code> exception.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>
<p>Handling exceptions is no different than writing SOLID code. The examples I gave above are mostly thought experiments, and every developer should write code that complies with their ACs. It’s always easy to go overboard, and as Software Engineers, we need to find the right balance of:</p>

<ol>
  <li>What should I do to make my code more readable?</li>
  <li>What should I do to produce code promptly?</li>
  <li>How should I protect myself from future ACs that the business might spring up on me?</li>
</ol>

<p>Arguably, our final solution needs to follow number 3 better. Some AC that treats different data source exceptions might appear in the future, and our pretty code will be tough to refactor to account for this new AC. So take my suggestions with a grain of salt.</p>]]></content><author><name></name></author><category term="Computer Science" /><category term="error handling" /><category term="kotlin" /><summary type="html"><![CDATA[Introduction Most of these insights came from this blog post and my subsequent attempts to try and implement Elizarov’s suggested patterns.]]></summary></entry><entry><title type="html">Solar Flares Forecasting</title><link href="/math/2022/05/22/Solar-Flare-Forecasting.html" rel="alternate" type="text/html" title="Solar Flares Forecasting" /><published>2022-05-22T00:00:00-04:00</published><updated>2022-05-22T00:00:00-04:00</updated><id>/math/2022/05/22/Solar-Flare-Forecasting</id><content type="html" xml:base="/math/2022/05/22/Solar-Flare-Forecasting.html"><![CDATA[<p>I find it silly to resort to machine learning for everything these days. Of course, it’s <em>absolutely</em> a valuable tool and helps mathematicians, computer scientists, alike. Particularly, I value the intersection of deductive reasoning with machine learning filling in the tedious work of crunching numbers and patterns.</p>

<p>Surely, we can say there’s some <em>function</em> that does what we want it to do. Is it a function we should define via machine learning? Likely it is due to the sheer complexity of the natural world.</p>

<p>That being said, my thesis delved into using machine learning to predict solar flares. It would be silly to try and model the photosphere using fundamental physics. It’s just too complex. I decided to visualize an active region as a collection of “things” that interact with one another. If I were to do this again, I would love to formalize that method a bit more.</p>

<p>Alas, <a href="https://github.com/lincketheo/thesis/blob/main/Thesis.pdf">here’s the paper</a>.</p>]]></content><author><name></name></author><category term="math" /><category term="solar flares" /><category term="machine learning" /><summary type="html"><![CDATA[I find it silly to resort to machine learning for everything these days. Of course, it’s absolutely a valuable tool and helps mathematicians, computer scientists, alike. Particularly, I value the intersection of deductive reasoning with machine learning filling in the tedious work of crunching numbers and patterns.]]></summary></entry></feed>
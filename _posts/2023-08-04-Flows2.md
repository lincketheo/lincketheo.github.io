---
title: An Elegant Solution to the Producer Consumer Problem
layout: single
date: 2023-08-04
categories: [computer science]
tags: [software engineering, asynchronous programming]
subtitle: How flows solve two specific problems
layout: post
difficulty: 1
published: true
---

## Introduction

On my quest to understand why flows are so idiomatic to kotlin and elegant in general, I've stumbled on the following two observations:

1. Coroutines / Structured concurrency seamlessly solves the following rule:
**For n blocking calls, the maximally performant program will consume n threads**. 

Rule (1) is seemingly obvious, until you think about a problem that has an unbounded number of blocking calls. Rule 1 also seems like it relates only to coroutines, but it is partly solved by the introduction of backpressure, which flows (and other patterns) make easier to implement using blocking consumers.

2. Flows allow us to seperate responsibility within producers / consumers because **the producer within a flow doesn't need to know what the consumer is doing**, allowing more than one programmer to work on n different consumer functions at the same time.

All the rules of **S** in **SOLID** design apply to rule (2). One might say the concept of a flow is more "_clean_".

## Flows - a Cleaner Solution to the Producer Consumer Problem

Recently I was working on a problem of loading images into memory while a Machine Learning model was
training. I wanted to load images from memory in an asynchronous way that didn't violate the
[Reactivity Manifesto](https://www.reactivemanifesto.org/). The basic premises are:

1. One producer blocks to retrieve data
2. One consumer blocks to transform said recieved data
3. It's faster to load n images than 1 image at a time

A first naive solution looks something this (I'll demo this first half in python's asyncio library because it
has some interesting properties):

```
async def produce():
        images = await get_images()
        for image in images:
                await consume(image)

async def consume(image):
        # do blocking consuming stuff

async def get_images():
        # do blocking fetching stuff
```

But recall from the reactive manifesto:

> The benefits of asynchronous processing would be negated if the communication of back pressure were synchronous

Although it seems like this code is async (it uses asyncio!), it really isn't. It's equivalent to writing:

```
def produce():
        # do blocking fetching stuff
        for image in images:
                # do blocking consuming stuff
```

which is just regular synchronous code! (One of the reasons the async await pattern can sort of trick you into
thinking you're writing async code). Let's try again and think about our code executing two threads at the same time
(effectively halving the time it takes to run):

```
images = []
async def produce():
        task = asyncio.create_task(get_images())
        for image in images:
                await consume(image)
        images = await task

async def consume(image):
        # do blocking consuming stuff

async def get_images():
        # do blocking fetching stuff
```

We see in this above example that the _first_ time around, images is empty, so the for loop around consume does
nothing, but the `task` from asyncio _begins fetching the next data before starting to process the current data_.
Sort of like the bounded buffer problem loading half the buffer even before the consumer is ready to start consuming
data from the buffer. That means our program execution is cut linearly (if the execution time of fetching
images is the same as consuming them, then the time of the second solution is theoretically
$\frac{1}{2} \times \text{original solution time} + \text{first image fetch}$).

There are other solutions using larger buffers etc, but effectively, if we have two blocking calls and two
threads, we're maximizing our resources (one of the reasons coroutines / structured concurrency
is so neat, because it maximizes the number of "threads" wrt the number of blocking calls).

A seemingly obvious rule is:

**If we have n blocking calls, we can maximize the benefits of asynchronous code by running on n threads at the same time**

It's simple, but we missed it in our first example.

## Using flows

The above example is meant to illustrate a solution to backpressure.

> **Backpressure** in software engineering is defined as the ability of a data consumer that cannot keep up with incoming data to send a signal to the data producer to slow down the rate of the data elements. [source](https://elizarov.medium.com/simple-design-of-kotlin-flow-4725e7398c4c)


> Traditional reactive streams design involves a back-channel to request more data from producers as needed. Management of this request protocol leads to notoriously difficult implementations even for simple operators. We do not see any of this complexity in the design of Kotlin flows, nor in the implementation of operators for them, yet Kotlin flows do support back-pressure. [source](https://elizarov.medium.com/simple-design-of-kotlin-flow-4725e7398c4c)

The previous example isn't reactive, but it solves the problem Elizarov highlighted in Flows. Back pressure
management is achieved by blocking the consumer until more data is needed. The producer waits for the consumer to
unblock before it fetches more data (half the buffer in the example above).

Flows don't offer anything new, other than a more elegant way (using the abstraction that is reactivity)
of implementing the code I wrote above (this is in kotlin now) so that we don't have to keep solving the problem
of n blockers getting n threads:

```
val image_producer = flow {
        var images = listOf()
        while (True) {
                val future_images = async(IO, fetch_images)
                image.forEach { emit(it) }
                future_images.onAwait { images = it }
        }
}

val pipeline = image_producer.forEach(consumer)

suspend fun fetch_images() {
        # do blocking recieving stuff
        return images
}

suspend fun consume(image) {
        # do blocking consuming stuff
}
```

This seemingly simple solution seems more complex! But consider more than one consumer:

```
val image_producer = flow {
        images = listOf()
        while (True) {
                future_images = async(IO) { fetch_images() }
                images.forEach { emit(it) }
                future_images.onAwait { images = it }
        }
}

val mapped = image_producer.map(consumer_1).map(consumer_2)

suspend fun fetch_images() {
        # do blocking recieving stuff
}

suspend fun consume_1(image) {
        # do blocking consuming stuff
        return new_data
}

suspend fun consume_2(image) {
        # do blocking consuming stuff
        return new_data
}
```

**The producer doesn't need any knowledge of what the consumer is going to do with the data**. The producer can just blindly emit data and wait for the consumer to notify that it's done (_of course, this can be done in other solutions such as the ones using semaphores, flows just feel a bit more intuitive).

In our first example, we would've had to add more function calls in the `produce` function to consume more than one image or output:

```
images = []
async def produce():
        task = asyncio.create_task(get_images())
        for image in images:
                new_images1 = await consume_1(image)
                new_images2 = await consume_2(image)
        images = await task

async def consume_1(image):
        # do blocking consuming stuff
        return new_images

async def consume_2(image):
        # do blocking consuming stuff
        return new_images

async def get_images():
        # do blocking fetching stuff
```

Of course, we're violating the rule that n blocking calls should get n threads and a clever engineer could
probably come up with a clever solution without reactivity, but the point remains that in this non reactive
style of thinking, we have to keep coming up with solutions to delegate our task to multiple threads! So
the real reason reactivity is solving the problem I just mentioned is single responsibility of producers / consumers, letting each producer / consumer get it's own thread!

Now, we can abstract out the producer and consumer and let multiple software engineers work on
each block of code seperately!

